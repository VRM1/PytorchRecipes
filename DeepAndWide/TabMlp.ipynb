{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineeth/anaconda3/envs/pytorch_03_2023/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep, TabResnet\n",
    "from pytorch_widedeep.metrics import Accuracy, F1Score\n",
    "from pytorch_widedeep.datasets import load_adult\n",
    "import warnings\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning, message=\"unclosed.*<zmq.*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.365981</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.812074</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.275557</td>\n",
       "      <td>0.356085</td>\n",
       "      <td>0.442651</td>\n",
       "      <td>-0.341942</td>\n",
       "      <td>-0.126784</td>\n",
       "      <td>-0.126411</td>\n",
       "      <td>-0.116564</td>\n",
       "      <td>0.013131</td>\n",
       "      <td>-0.293382</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.674276</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.246020</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.588044</td>\n",
       "      <td>-0.629290</td>\n",
       "      <td>-0.555651</td>\n",
       "      <td>-0.257657</td>\n",
       "      <td>0.556969</td>\n",
       "      <td>0.012684</td>\n",
       "      <td>-0.176631</td>\n",
       "      <td>0.065363</td>\n",
       "      <td>-0.119114</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.136720</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.249166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.419745</td>\n",
       "      <td>-0.403667</td>\n",
       "      <td>-0.386071</td>\n",
       "      <td>-0.251378</td>\n",
       "      <td>-0.083382</td>\n",
       "      <td>-0.205927</td>\n",
       "      <td>-0.276146</td>\n",
       "      <td>-0.281409</td>\n",
       "      <td>-0.270881</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.905498</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055816</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.091853</td>\n",
       "      <td>-0.499282</td>\n",
       "      <td>-0.484068</td>\n",
       "      <td>-0.221191</td>\n",
       "      <td>-0.161505</td>\n",
       "      <td>-0.231599</td>\n",
       "      <td>-0.276146</td>\n",
       "      <td>-0.281409</td>\n",
       "      <td>-0.272231</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.250611</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.812074</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.667849</td>\n",
       "      <td>-0.658140</td>\n",
       "      <td>-0.647804</td>\n",
       "      <td>-0.341942</td>\n",
       "      <td>-0.256990</td>\n",
       "      <td>-0.279762</td>\n",
       "      <td>-0.288913</td>\n",
       "      <td>-0.294893</td>\n",
       "      <td>-0.293382</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE       AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0  -0.365981    0          0         0 -0.812074      4      0      1      1   \n",
       "1  -0.674276    0          0         1 -1.246020      1      2      0      0   \n",
       "2  -1.136720    1          0         1  1.249166      2      1      1      1   \n",
       "3  -0.905498    1          0         0  0.055816      2      1      1      1   \n",
       "4   0.250611    1          1         1 -0.812074      3      3      3      2   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0      1  ...   0.275557   0.356085   0.442651 -0.341942 -0.126784 -0.126411   \n",
       "1      2  ...  -0.588044  -0.629290  -0.555651 -0.257657  0.556969  0.012684   \n",
       "2      1  ...  -0.419745  -0.403667  -0.386071 -0.251378 -0.083382 -0.205927   \n",
       "3      1  ...   0.091853  -0.499282  -0.484068 -0.221191 -0.161505 -0.231599   \n",
       "4      0  ...  -0.667849  -0.658140  -0.647804 -0.341942 -0.256990 -0.279762   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0 -0.116564  0.013131 -0.293382                           1  \n",
       "1 -0.176631  0.065363 -0.119114                           0  \n",
       "2 -0.276146 -0.281409 -0.270881                           0  \n",
       "3 -0.276146 -0.281409 -0.272231                           1  \n",
       "4 -0.288913 -0.294893 -0.293382                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lukup = {'defaultCredit':'default.payment.next.month', 'bank':'y'}\n",
    "name = 'defaultCredit'\n",
    "label = lukup[name]\n",
    "fold = 0\n",
    "train_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/TabularDataModels/Dataset/{}/fold{}/train/data.csv'.format(name, fold))\n",
    "valid_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/TabularDataModels/Dataset/{}/fold{}/valid/data.csv'.format(name, fold))\n",
    "test_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/TabularDataModels/Dataset/{}/fold{}/test/data.csv'.format(name, fold))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'column set up'\n",
    "wide_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "\n",
    "cat_embed_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "continuous_cols = [\"LIMIT_BAL\", \"BILL_AMT1\", \"BILL_AMT1\", \"BILL_AMT2\", \\\n",
    "     \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", 'PAY_AMT1', 'PAY_AMT1',\\\n",
    "        'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "target = \"default.payment.next.month\"\n",
    "target = train_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(train_df)\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols  # type: ignore[arg-type]\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.30413785, -0.12509114, -0.28102013, ..., -0.07244054,\n",
       "       -0.0151664 , -0.0151664 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tab[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SEX': 0,\n",
       " 'EDUCATION': 1,\n",
       " 'MARRIAGE': 2,\n",
       " 'PAY_0': 3,\n",
       " 'PAY_2': 4,\n",
       " 'PAY_3': 5,\n",
       " 'PAY_4': 6,\n",
       " 'PAY_5': 7,\n",
       " 'PAY_6': 8,\n",
       " 'LIMIT_BAL': 9,\n",
       " 'BILL_AMT1': 11,\n",
       " 'BILL_AMT2': 12,\n",
       " 'BILL_AMT3': 13,\n",
       " 'BILL_AMT4': 14,\n",
       " 'BILL_AMT5': 15,\n",
       " 'BILL_AMT6': 16,\n",
       " 'PAY_AMT1': 18,\n",
       " 'PAY_AMT2': 19,\n",
       " 'PAY_AMT3': 20,\n",
       " 'PAY_AMT4': 21,\n",
       " 'PAY_AMT5': 22,\n",
       " 'PAY_AMT6': 23}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_preprocessor.column_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "wide = Wide(input_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "# tab_mlp = TabMlp(\n",
    "#     column_idx=tab_preprocessor.column_idx,\n",
    "#     cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "#     continuous_cols=continuous_cols,\n",
    "#     mlp_hidden_dims=[400, 200],\n",
    "#     mlp_dropout=0.5,\n",
    "#     mlp_activation=\"leaky_relu\",\n",
    "#     embed_continuous=True,\n",
    "#     mlp_batchnorm=True\n",
    "# )\n",
    "\n",
    "tab_mlp = TabResnet(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[400, 200],\n",
    "    mlp_dropout=0.5,\n",
    "    mlp_activation=\"leaky_relu\"\n",
    ")\n",
    "model = WideDeep(wide=wide, deeptabular=tab_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TabResnet(\n",
       "  (cat_and_cont_embed): DiffSizeCatAndContEmbeddings(\n",
       "    (cat_embed): DiffSizeCatEmbeddings(\n",
       "      (embed_layers): ModuleDict(\n",
       "        (emb_layer_SEX): Embedding(3, 2, padding_idx=0)\n",
       "        (emb_layer_EDUCATION): Embedding(8, 5, padding_idx=0)\n",
       "        (emb_layer_MARRIAGE): Embedding(5, 3, padding_idx=0)\n",
       "        (emb_layer_PAY_0): Embedding(12, 6, padding_idx=0)\n",
       "        (emb_layer_PAY_2): Embedding(12, 6, padding_idx=0)\n",
       "        (emb_layer_PAY_3): Embedding(12, 6, padding_idx=0)\n",
       "        (emb_layer_PAY_4): Embedding(12, 6, padding_idx=0)\n",
       "        (emb_layer_PAY_5): Embedding(11, 6, padding_idx=0)\n",
       "        (emb_layer_PAY_6): Embedding(11, 6, padding_idx=0)\n",
       "      )\n",
       "      (embedding_dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (cont_norm): BatchNorm1d(15, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (encoder): DenseResnet(\n",
       "    (dense_resnet): Sequential(\n",
       "      (lin_inp): Linear(in_features=61, out_features=200, bias=False)\n",
       "      (bn_inp): BatchNorm1d(200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (block_0): BasicBlock(\n",
       "        (resize): Sequential(\n",
       "          (0): Linear(in_features=200, out_features=100, bias=False)\n",
       "          (1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (lin1): Linear(in_features=200, out_features=100, bias=False)\n",
       "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "        (lin2): Linear(in_features=100, out_features=100, bias=False)\n",
       "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (block_1): BasicBlock(\n",
       "        (lin1): Linear(in_features=100, out_features=100, bias=False)\n",
       "        (bn1): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (leaky_relu): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "        (dp): Dropout(p=0.1, inplace=False)\n",
       "        (lin2): Linear(in_features=100, out_features=100, bias=False)\n",
       "        (bn2): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlp): MLP(\n",
       "    (mlp): Sequential(\n",
       "      (dense_layer_0): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Linear(in_features=100, out_features=400, bias=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "      (dense_layer_1): Sequential(\n",
       "        (0): Dropout(p=0.5, inplace=False)\n",
       "        (1): Linear(in_features=400, out_features=200, bias=True)\n",
       "        (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tab_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 69.68it/s, loss=0.541, metrics={'BinaryAUROC': 0.6305, 'f1': 0.2789, 'BinaryAveragePrecision': 0.3552}] \n",
      "epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.67it/s, loss=0.496, metrics={'BinaryAUROC': 0.6896, 'f1': 0.339, 'BinaryAveragePrecision': 0.4197}]  \n",
      "epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.74it/s, loss=0.482, metrics={'BinaryAUROC': 0.7071, 'f1': 0.3467, 'BinaryAveragePrecision': 0.4441}] \n",
      "epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 79.55it/s, loss=0.472, metrics={'BinaryAUROC': 0.7229, 'f1': 0.369, 'BinaryAveragePrecision': 0.4617}]  \n",
      "epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 71.66it/s, loss=0.467, metrics={'BinaryAUROC': 0.7275, 'f1': 0.3702, 'BinaryAveragePrecision': 0.4706}] \n",
      "epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.80it/s, loss=0.462, metrics={'BinaryAUROC': 0.7353, 'f1': 0.3772, 'BinaryAveragePrecision': 0.4793}] \n",
      "epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.88it/s, loss=0.457, metrics={'BinaryAUROC': 0.7404, 'f1': 0.3828, 'BinaryAveragePrecision': 0.4869}] \n",
      "epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.34it/s, loss=0.454, metrics={'BinaryAUROC': 0.7448, 'f1': 0.3904, 'BinaryAveragePrecision': 0.4926}] \n",
      "epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.97it/s, loss=0.452, metrics={'BinaryAUROC': 0.7476, 'f1': 0.3933, 'BinaryAveragePrecision': 0.5003}] \n",
      "epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.94it/s, loss=0.449, metrics={'BinaryAUROC': 0.7516, 'f1': 0.4049, 'BinaryAveragePrecision': 0.5042}] \n",
      "epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.82it/s, loss=0.447, metrics={'BinaryAUROC': 0.7512, 'f1': 0.4095, 'BinaryAveragePrecision': 0.5053}] \n",
      "epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.31it/s, loss=0.445, metrics={'BinaryAUROC': 0.7566, 'f1': 0.4155, 'BinaryAveragePrecision': 0.5128}] \n",
      "epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 77.01it/s, loss=0.444, metrics={'BinaryAUROC': 0.7581, 'f1': 0.419, 'BinaryAveragePrecision': 0.5081}]  \n",
      "epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 71.77it/s, loss=0.441, metrics={'BinaryAUROC': 0.7612, 'f1': 0.4193, 'BinaryAveragePrecision': 0.5164}]\n",
      "epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.07it/s, loss=0.439, metrics={'BinaryAUROC': 0.7645, 'f1': 0.4287, 'BinaryAveragePrecision': 0.5278}] \n",
      "epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.49it/s, loss=0.439, metrics={'BinaryAUROC': 0.7656, 'f1': 0.4294, 'BinaryAveragePrecision': 0.5196}] \n",
      "epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.34it/s, loss=0.437, metrics={'BinaryAUROC': 0.7657, 'f1': 0.4378, 'BinaryAveragePrecision': 0.5258}] \n",
      "epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.26it/s, loss=0.437, metrics={'BinaryAUROC': 0.7661, 'f1': 0.4369, 'BinaryAveragePrecision': 0.5236}] \n",
      "epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.36it/s, loss=0.436, metrics={'BinaryAUROC': 0.7693, 'f1': 0.4354, 'BinaryAveragePrecision': 0.5269}] \n",
      "epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.83it/s, loss=0.437, metrics={'BinaryAUROC': 0.7661, 'f1': 0.4314, 'BinaryAveragePrecision': 0.5276}] \n",
      "epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.91it/s, loss=0.433, metrics={'BinaryAUROC': 0.7722, 'f1': 0.4471, 'BinaryAveragePrecision': 0.5343}] \n",
      "epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.43it/s, loss=0.434, metrics={'BinaryAUROC': 0.7698, 'f1': 0.4402, 'BinaryAveragePrecision': 0.5338}] \n",
      "epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.33it/s, loss=0.434, metrics={'BinaryAUROC': 0.7707, 'f1': 0.4434, 'BinaryAveragePrecision': 0.5339}] \n",
      "epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.47it/s, loss=0.432, metrics={'BinaryAUROC': 0.7722, 'f1': 0.4456, 'BinaryAveragePrecision': 0.541}]  \n",
      "epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.52it/s, loss=0.433, metrics={'BinaryAUROC': 0.7724, 'f1': 0.4456, 'BinaryAveragePrecision': 0.5345}] \n",
      "epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.44it/s, loss=0.432, metrics={'BinaryAUROC': 0.7737, 'f1': 0.4453, 'BinaryAveragePrecision': 0.5369}] \n",
      "epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.16it/s, loss=0.433, metrics={'BinaryAUROC': 0.7726, 'f1': 0.4528, 'BinaryAveragePrecision': 0.5349}] \n",
      "epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.60it/s, loss=0.432, metrics={'BinaryAUROC': 0.7726, 'f1': 0.4527, 'BinaryAveragePrecision': 0.537}]  \n",
      "epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.30it/s, loss=0.431, metrics={'BinaryAUROC': 0.7753, 'f1': 0.4505, 'BinaryAveragePrecision': 0.5422}] \n",
      "epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.17it/s, loss=0.431, metrics={'BinaryAUROC': 0.7752, 'f1': 0.4516, 'BinaryAveragePrecision': 0.5442}] \n",
      "epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.27it/s, loss=0.431, metrics={'BinaryAUROC': 0.7754, 'f1': 0.4537, 'BinaryAveragePrecision': 0.5413}] \n",
      "epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.89it/s, loss=0.43, metrics={'BinaryAUROC': 0.777, 'f1': 0.4522, 'BinaryAveragePrecision': 0.5444}]   \n",
      "epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.21it/s, loss=0.431, metrics={'BinaryAUROC': 0.7748, 'f1': 0.4546, 'BinaryAveragePrecision': 0.5414}] \n",
      "epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.02it/s, loss=0.429, metrics={'BinaryAUROC': 0.7779, 'f1': 0.4604, 'BinaryAveragePrecision': 0.5436}] \n",
      "epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.44it/s, loss=0.428, metrics={'BinaryAUROC': 0.7784, 'f1': 0.4602, 'BinaryAveragePrecision': 0.5459}] \n",
      "epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.72it/s, loss=0.43, metrics={'BinaryAUROC': 0.7766, 'f1': 0.4572, 'BinaryAveragePrecision': 0.5413}]  \n",
      "epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.15it/s, loss=0.429, metrics={'BinaryAUROC': 0.7773, 'f1': 0.4658, 'BinaryAveragePrecision': 0.5448}] \n",
      "epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.74it/s, loss=0.428, metrics={'BinaryAUROC': 0.7782, 'f1': 0.4572, 'BinaryAveragePrecision': 0.5469}] \n",
      "epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.65it/s, loss=0.429, metrics={'BinaryAUROC': 0.7766, 'f1': 0.4593, 'BinaryAveragePrecision': 0.5441}] \n",
      "epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.55it/s, loss=0.428, metrics={'BinaryAUROC': 0.7783, 'f1': 0.464, 'BinaryAveragePrecision': 0.5457}]  \n",
      "epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.09it/s, loss=0.428, metrics={'BinaryAUROC': 0.7795, 'f1': 0.4654, 'BinaryAveragePrecision': 0.5491}] \n",
      "epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.28it/s, loss=0.428, metrics={'BinaryAUROC': 0.7783, 'f1': 0.4618, 'BinaryAveragePrecision': 0.5498}] \n",
      "epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.78it/s, loss=0.427, metrics={'BinaryAUROC': 0.7802, 'f1': 0.4618, 'BinaryAveragePrecision': 0.5505}] \n",
      "epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.90it/s, loss=0.428, metrics={'BinaryAUROC': 0.7792, 'f1': 0.4609, 'BinaryAveragePrecision': 0.5494}] \n",
      "epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.92it/s, loss=0.427, metrics={'BinaryAUROC': 0.7797, 'f1': 0.4662, 'BinaryAveragePrecision': 0.5508}] \n",
      "epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.74it/s, loss=0.427, metrics={'BinaryAUROC': 0.7806, 'f1': 0.4628, 'BinaryAveragePrecision': 0.5516}] \n",
      "epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.55it/s, loss=0.428, metrics={'BinaryAUROC': 0.7795, 'f1': 0.4617, 'BinaryAveragePrecision': 0.5478}] \n",
      "epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 77.32it/s, loss=0.426, metrics={'BinaryAUROC': 0.7811, 'f1': 0.4632, 'BinaryAveragePrecision': 0.5507}] \n",
      "epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.39it/s, loss=0.426, metrics={'BinaryAUROC': 0.7805, 'f1': 0.4607, 'BinaryAveragePrecision': 0.5542}] \n",
      "epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.64it/s, loss=0.427, metrics={'BinaryAUROC': 0.7796, 'f1': 0.4617, 'BinaryAveragePrecision': 0.5524}] \n",
      "epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.58it/s, loss=0.426, metrics={'BinaryAUROC': 0.7831, 'f1': 0.4665, 'BinaryAveragePrecision': 0.5516}] \n",
      "epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.50it/s, loss=0.426, metrics={'BinaryAUROC': 0.7817, 'f1': 0.4678, 'BinaryAveragePrecision': 0.5537}] \n",
      "epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.04it/s, loss=0.425, metrics={'BinaryAUROC': 0.7805, 'f1': 0.4677, 'BinaryAveragePrecision': 0.5529}] \n",
      "epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.57it/s, loss=0.426, metrics={'BinaryAUROC': 0.7816, 'f1': 0.4671, 'BinaryAveragePrecision': 0.5524}] \n",
      "epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.46it/s, loss=0.425, metrics={'BinaryAUROC': 0.7805, 'f1': 0.4658, 'BinaryAveragePrecision': 0.5572}] \n",
      "epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 77.35it/s, loss=0.424, metrics={'BinaryAUROC': 0.7828, 'f1': 0.4725, 'BinaryAveragePrecision': 0.5577}] \n",
      "epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.94it/s, loss=0.425, metrics={'BinaryAUROC': 0.7834, 'f1': 0.4681, 'BinaryAveragePrecision': 0.5541}] \n",
      "epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.31it/s, loss=0.424, metrics={'BinaryAUROC': 0.7844, 'f1': 0.4676, 'BinaryAveragePrecision': 0.5552}] \n",
      "epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 71.50it/s, loss=0.424, metrics={'BinaryAUROC': 0.7827, 'f1': 0.4723, 'BinaryAveragePrecision': 0.5617}] \n",
      "epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.35it/s, loss=0.425, metrics={'BinaryAUROC': 0.7835, 'f1': 0.4654, 'BinaryAveragePrecision': 0.5569}] \n",
      "epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.46it/s, loss=0.425, metrics={'BinaryAUROC': 0.7825, 'f1': 0.4652, 'BinaryAveragePrecision': 0.5564}] \n",
      "epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.50it/s, loss=0.423, metrics={'BinaryAUROC': 0.7859, 'f1': 0.4663, 'BinaryAveragePrecision': 0.5606}] \n",
      "epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.08it/s, loss=0.425, metrics={'BinaryAUROC': 0.783, 'f1': 0.4652, 'BinaryAveragePrecision': 0.5582}]  \n",
      "epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.78it/s, loss=0.425, metrics={'BinaryAUROC': 0.7817, 'f1': 0.4656, 'BinaryAveragePrecision': 0.5597}] \n",
      "epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.80it/s, loss=0.424, metrics={'BinaryAUROC': 0.7827, 'f1': 0.4687, 'BinaryAveragePrecision': 0.5597}] \n",
      "epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.55it/s, loss=0.425, metrics={'BinaryAUROC': 0.7831, 'f1': 0.47, 'BinaryAveragePrecision': 0.5581}]   \n",
      "epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.19it/s, loss=0.424, metrics={'BinaryAUROC': 0.7841, 'f1': 0.4688, 'BinaryAveragePrecision': 0.5631}] \n",
      "epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.78it/s, loss=0.425, metrics={'BinaryAUROC': 0.783, 'f1': 0.4659, 'BinaryAveragePrecision': 0.5577}]  \n",
      "epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.68it/s, loss=0.424, metrics={'BinaryAUROC': 0.7845, 'f1': 0.4722, 'BinaryAveragePrecision': 0.5568}] \n",
      "epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.49it/s, loss=0.424, metrics={'BinaryAUROC': 0.7844, 'f1': 0.4694, 'BinaryAveragePrecision': 0.5566}] \n",
      "epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.72it/s, loss=0.424, metrics={'BinaryAUROC': 0.785, 'f1': 0.4646, 'BinaryAveragePrecision': 0.5577}]  \n",
      "epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.76it/s, loss=0.423, metrics={'BinaryAUROC': 0.7857, 'f1': 0.4711, 'BinaryAveragePrecision': 0.5642}] \n",
      "epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 70.50it/s, loss=0.423, metrics={'BinaryAUROC': 0.7856, 'f1': 0.4694, 'BinaryAveragePrecision': 0.5632}] \n",
      "epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.15it/s, loss=0.424, metrics={'BinaryAUROC': 0.7851, 'f1': 0.4647, 'BinaryAveragePrecision': 0.5629}] \n",
      "epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.16it/s, loss=0.424, metrics={'BinaryAUROC': 0.7856, 'f1': 0.4661, 'BinaryAveragePrecision': 0.5596}] \n",
      "epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.66it/s, loss=0.421, metrics={'BinaryAUROC': 0.7882, 'f1': 0.4692, 'BinaryAveragePrecision': 0.5631}] \n",
      "epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.26it/s, loss=0.422, metrics={'BinaryAUROC': 0.7867, 'f1': 0.4703, 'BinaryAveragePrecision': 0.5651}] \n",
      "epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.81it/s, loss=0.422, metrics={'BinaryAUROC': 0.7863, 'f1': 0.4725, 'BinaryAveragePrecision': 0.5614}] \n",
      "epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.95it/s, loss=0.421, metrics={'BinaryAUROC': 0.7876, 'f1': 0.4704, 'BinaryAveragePrecision': 0.5667}] \n",
      "epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.24it/s, loss=0.421, metrics={'BinaryAUROC': 0.7886, 'f1': 0.4742, 'BinaryAveragePrecision': 0.5634}] \n",
      "epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.95it/s, loss=0.423, metrics={'BinaryAUROC': 0.7848, 'f1': 0.4699, 'BinaryAveragePrecision': 0.5637}] \n",
      "epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.08it/s, loss=0.422, metrics={'BinaryAUROC': 0.7868, 'f1': 0.4696, 'BinaryAveragePrecision': 0.5669}] \n",
      "epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.97it/s, loss=0.422, metrics={'BinaryAUROC': 0.7867, 'f1': 0.4655, 'BinaryAveragePrecision': 0.5653}] \n",
      "epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.51it/s, loss=0.422, metrics={'BinaryAUROC': 0.7879, 'f1': 0.4685, 'BinaryAveragePrecision': 0.5638}] \n",
      "epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.59it/s, loss=0.423, metrics={'BinaryAUROC': 0.786, 'f1': 0.4659, 'BinaryAveragePrecision': 0.5639}]  \n",
      "epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.40it/s, loss=0.422, metrics={'BinaryAUROC': 0.7856, 'f1': 0.4712, 'BinaryAveragePrecision': 0.5636}] \n",
      "epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 72.95it/s, loss=0.421, metrics={'BinaryAUROC': 0.7878, 'f1': 0.4708, 'BinaryAveragePrecision': 0.5683}] \n",
      "epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.10it/s, loss=0.421, metrics={'BinaryAUROC': 0.7865, 'f1': 0.4696, 'BinaryAveragePrecision': 0.5663}] \n",
      "epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.21it/s, loss=0.422, metrics={'BinaryAUROC': 0.7876, 'f1': 0.4703, 'BinaryAveragePrecision': 0.5667}] \n",
      "epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.79it/s, loss=0.421, metrics={'BinaryAUROC': 0.788, 'f1': 0.4747, 'BinaryAveragePrecision': 0.5654}]  \n",
      "epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.00it/s, loss=0.422, metrics={'BinaryAUROC': 0.7868, 'f1': 0.4681, 'BinaryAveragePrecision': 0.5629}] \n",
      "epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.18it/s, loss=0.421, metrics={'BinaryAUROC': 0.7887, 'f1': 0.4722, 'BinaryAveragePrecision': 0.5647}] \n",
      "epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.70it/s, loss=0.421, metrics={'BinaryAUROC': 0.7884, 'f1': 0.4726, 'BinaryAveragePrecision': 0.5693}] \n",
      "epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 76.51it/s, loss=0.421, metrics={'BinaryAUROC': 0.7886, 'f1': 0.4664, 'BinaryAveragePrecision': 0.5641}] \n",
      "epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 75.52it/s, loss=0.419, metrics={'BinaryAUROC': 0.7906, 'f1': 0.4697, 'BinaryAveragePrecision': 0.5683}] \n",
      "epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.26it/s, loss=0.42, metrics={'BinaryAUROC': 0.7886, 'f1': 0.4717, 'BinaryAveragePrecision': 0.5693}]  \n",
      "epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.61it/s, loss=0.42, metrics={'BinaryAUROC': 0.7893, 'f1': 0.4692, 'BinaryAveragePrecision': 0.5678}]  \n",
      "epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 71.27it/s, loss=0.42, metrics={'BinaryAUROC': 0.7892, 'f1': 0.4718, 'BinaryAveragePrecision': 0.5709}] \n",
      "epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 74.98it/s, loss=0.421, metrics={'BinaryAUROC': 0.7878, 'f1': 0.4698, 'BinaryAveragePrecision': 0.5663}] \n",
      "epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 73.86it/s, loss=0.42, metrics={'BinaryAUROC': 0.7882, 'f1': 0.4744, 'BinaryAveragePrecision': 0.5673}]  \n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "trainer = Trainer(model, objective=\"binary\", accelerator=\"gpu\",\\\n",
    "                  metrics=[AUROC(task='binary'), F1Score, AveragePrecision(task='binary')])\n",
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 49.16it/s]\n",
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 45.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict on test\n",
    "X_wide_te = wide_preprocessor.transform(test_df)\n",
    "X_tab_te = tab_preprocessor.transform(test_df)\n",
    "preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)\n",
    "pred_probs = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.7925768688745115\n",
      "PrecisionRecall-AUC:0.5728320749314739\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "target = lukup[name]\n",
    "y = test_df[target].values\n",
    "print(\"ROC-AUC:{}\".format(roc_auc_score(y, pred_probs[:, 1])))\n",
    "print(\"PrecisionRecall-AUC:{}\".format(average_precision_score(y, pred_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Tab2Vec\n",
    "t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor)\n",
    "X_vec, y = t2v.transform(train_df, target_col=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4172393e-01, -2.9816014e-01,  2.1386049e+00, ...,\n",
       "         5.1322991e-01,  8.6423665e-01, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         5.5625874e-01,  4.9738172e-01,  8.4056890e-01],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         6.8739414e-01, -1.5589465e-01,  2.2773863e-01],\n",
       "       ...,\n",
       "       [-1.4172393e-01, -2.9816014e-01, -4.5036829e-01, ...,\n",
       "        -5.1062021e-02, -8.0248013e-02, -8.1868723e-02],\n",
       "       [-1.8721935e+00,  8.3828169e-01,  2.1386049e+00, ...,\n",
       "         3.1717189e-02,  1.0486166e-03, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "        -6.6634342e-02, -7.7423006e-02, -9.5329911e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_03_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
