{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineeth/anaconda3/envs/pytorch_03_2023/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy, F1Score\n",
    "from pytorch_widedeep.datasets import load_adult\n",
    "from pytorch_widedeep.models import FTTransformer\n",
    "import warnings\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning, message=\"unclosed.*<zmq.*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.134759</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.029047</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428605</td>\n",
       "      <td>-0.369043</td>\n",
       "      <td>-0.239816</td>\n",
       "      <td>0.503316</td>\n",
       "      <td>-0.039980</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>0.194622</td>\n",
       "      <td>0.536758</td>\n",
       "      <td>-0.180878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.483795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.357652</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>-0.525994</td>\n",
       "      <td>-0.512933</td>\n",
       "      <td>-0.070252</td>\n",
       "      <td>-0.126784</td>\n",
       "      <td>2.543032</td>\n",
       "      <td>0.228134</td>\n",
       "      <td>0.230763</td>\n",
       "      <td>0.436037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.674276</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.378129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.561493</td>\n",
       "      <td>0.571863</td>\n",
       "      <td>-0.160815</td>\n",
       "      <td>-0.083165</td>\n",
       "      <td>-0.154810</td>\n",
       "      <td>0.330267</td>\n",
       "      <td>-0.314136</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.751350</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.249166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>0.504203</td>\n",
       "      <td>0.545752</td>\n",
       "      <td>-0.148740</td>\n",
       "      <td>-0.109423</td>\n",
       "      <td>-0.132091</td>\n",
       "      <td>-0.132522</td>\n",
       "      <td>-0.107827</td>\n",
       "      <td>-0.141502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.365981</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126163</td>\n",
       "      <td>1.336657</td>\n",
       "      <td>1.378068</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>-0.061681</td>\n",
       "      <td>-0.041216</td>\n",
       "      <td>0.196217</td>\n",
       "      <td>-0.117776</td>\n",
       "      <td>-0.067081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE       AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0  -0.134759    0          1         1 -1.029047      2      1      1      1   \n",
       "1   1.483795    1          0         1  1.357652      2      1      1      1   \n",
       "2  -0.674276    1          0         1 -0.378129      0      1      1      1   \n",
       "3  -0.751350    0          2         0  1.249166      2      1      1      1   \n",
       "4  -0.365981    1          1         1  0.598248      2      1      1      1   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0      1  ...  -0.428605  -0.369043  -0.239816  0.503316 -0.039980 -0.012818   \n",
       "1      1  ...   0.579916  -0.525994  -0.512933 -0.070252 -0.126784  2.543032   \n",
       "2      1  ...   0.374450   0.561493   0.571863 -0.160815 -0.083165 -0.154810   \n",
       "3      1  ...   0.425778   0.504203   0.545752 -0.148740 -0.109423 -0.132091   \n",
       "4      1  ...   1.126163   1.336657   1.378068  0.020312 -0.061681 -0.041216   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0  0.194622  0.536758 -0.180878                           0  \n",
       "1  0.228134  0.230763  0.436037                           0  \n",
       "2  0.330267 -0.314136 -0.012122                           1  \n",
       "3 -0.132522 -0.107827 -0.141502                           0  \n",
       "4  0.196217 -0.117776 -0.067081                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lukup = {'defaultCredit':'default.payment.next.month', 'bank':'y'}\n",
    "name = 'defaultCredit'\n",
    "label = lukup[name]\n",
    "fold = 1\n",
    "train_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/train/data.csv'.format(name, fold))\n",
    "valid_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/valid/data.csv'.format(name, fold))\n",
    "test_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/test/data.csv'.format(name, fold))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'column set up'\n",
    "wide_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "\n",
    "cat_embed_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "continuous_cols = [\"LIMIT_BAL\", \"BILL_AMT1\", \"BILL_AMT1\", \"BILL_AMT2\", \\\n",
    "     \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", 'PAY_AMT1', 'PAY_AMT1',\\\n",
    "        'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "target = \"default.payment.next.month\"\n",
    "target = train_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(train_df)\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols  # type: ignore[arg-type]\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wide = Wide(input_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "# Define the FTTransformer model\n",
    "cat_embed_input = tab_preprocessor.cat_embed_input\n",
    "column_idx = tab_preprocessor.column_idx\n",
    "deeptabular = FTTransformer(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    n_heads=8, \n",
    "    n_blocks=6,\n",
    "    use_qkv_bias=True, \n",
    "    attn_dropout=0.1, \n",
    "    ff_dropout=0.1,\n",
    ")\n",
    "\n",
    "model = WideDeep(wide=wide, deeptabular=deeptabular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WideDeep(\n",
       "  (wide): Wide(\n",
       "    (wide_linear): Embedding(77, 1, padding_idx=0)\n",
       "  )\n",
       "  (deeptabular): Sequential(\n",
       "    (0): FTTransformer(\n",
       "      (cat_and_cont_embed): SameSizeCatAndContEmbeddings(\n",
       "        (cat_embed): SameSizeCatEmbeddings(\n",
       "          (embed): Embedding(77, 64, padding_idx=0)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (cont_norm): Identity()\n",
       "        (cont_embed): ContEmbeddings(15, 64, embed_dropout=0.1, use_bias=True)\n",
       "      )\n",
       "      (encoder): Sequential(\n",
       "        (fttransformer_block0): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (fttransformer_block1): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (fttransformer_block2): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (fttransformer_block3): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (fttransformer_block4): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (fttransformer_block5): FTTransformerEncoder(\n",
       "          (attn): LinearAttention(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (qkv_proj): Linear(in_features=64, out_features=192, bias=True)\n",
       "            (out_proj): Linear(in_features=64, out_features=64, bias=True)\n",
       "          )\n",
       "          (ff): FeedForward(\n",
       "            (w_1): Linear(in_features=64, out_features=170, bias=True)\n",
       "            (w_2): Linear(in_features=85, out_features=64, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (activation): REGLU()\n",
       "          )\n",
       "          (attn_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (ff_normadd): NormAdd(\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "            (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): Linear(in_features=1536, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 29.62it/s, loss=0.486, metrics={'BinaryAUROC': 0.7109, 'f1': 0.4196, 'BinaryAveragePrecision': 0.4553}]\n",
      "epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.10it/s, loss=0.443, metrics={'BinaryAUROC': 0.7567, 'f1': 0.4538, 'BinaryAveragePrecision': 0.5212}]\n",
      "epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.17it/s, loss=0.439, metrics={'BinaryAUROC': 0.7642, 'f1': 0.4676, 'BinaryAveragePrecision': 0.5329}]\n",
      "epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.25it/s, loss=0.436, metrics={'BinaryAUROC': 0.7686, 'f1': 0.4699, 'BinaryAveragePrecision': 0.5378}]\n",
      "epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 37.04it/s, loss=0.434, metrics={'BinaryAUROC': 0.7705, 'f1': 0.4749, 'BinaryAveragePrecision': 0.5408}]\n",
      "epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.62it/s, loss=0.433, metrics={'BinaryAUROC': 0.7733, 'f1': 0.4795, 'BinaryAveragePrecision': 0.5464}]\n",
      "epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.09it/s, loss=0.432, metrics={'BinaryAUROC': 0.7732, 'f1': 0.4801, 'BinaryAveragePrecision': 0.5506}]\n",
      "epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.52it/s, loss=0.431, metrics={'BinaryAUROC': 0.7752, 'f1': 0.4829, 'BinaryAveragePrecision': 0.5527}]\n",
      "epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.38it/s, loss=0.429, metrics={'BinaryAUROC': 0.7788, 'f1': 0.4773, 'BinaryAveragePrecision': 0.5554}]\n",
      "epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.54it/s, loss=0.427, metrics={'BinaryAUROC': 0.7815, 'f1': 0.4853, 'BinaryAveragePrecision': 0.5612}]\n",
      "epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.49it/s, loss=0.427, metrics={'BinaryAUROC': 0.7822, 'f1': 0.4847, 'BinaryAveragePrecision': 0.5564}]\n",
      "epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.54it/s, loss=0.425, metrics={'BinaryAUROC': 0.7846, 'f1': 0.4802, 'BinaryAveragePrecision': 0.5608}]\n",
      "epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.27it/s, loss=0.423, metrics={'BinaryAUROC': 0.7877, 'f1': 0.4906, 'BinaryAveragePrecision': 0.567}] \n",
      "epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.02it/s, loss=0.423, metrics={'BinaryAUROC': 0.7866, 'f1': 0.4942, 'BinaryAveragePrecision': 0.5663}]\n",
      "epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.76it/s, loss=0.42, metrics={'BinaryAUROC': 0.7897, 'f1': 0.4924, 'BinaryAveragePrecision': 0.5762}] \n",
      "epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.09it/s, loss=0.419, metrics={'BinaryAUROC': 0.7913, 'f1': 0.4971, 'BinaryAveragePrecision': 0.5778}]\n",
      "epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.75it/s, loss=0.418, metrics={'BinaryAUROC': 0.7931, 'f1': 0.5008, 'BinaryAveragePrecision': 0.5794}]\n",
      "epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.04it/s, loss=0.417, metrics={'BinaryAUROC': 0.794, 'f1': 0.5017, 'BinaryAveragePrecision': 0.584}]  \n",
      "epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.98it/s, loss=0.415, metrics={'BinaryAUROC': 0.7959, 'f1': 0.5062, 'BinaryAveragePrecision': 0.5833}]\n",
      "epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.42it/s, loss=0.413, metrics={'BinaryAUROC': 0.7985, 'f1': 0.5139, 'BinaryAveragePrecision': 0.5899}]\n",
      "epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.21it/s, loss=0.412, metrics={'BinaryAUROC': 0.8003, 'f1': 0.5087, 'BinaryAveragePrecision': 0.5936}]\n",
      "epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.60it/s, loss=0.412, metrics={'BinaryAUROC': 0.8007, 'f1': 0.5041, 'BinaryAveragePrecision': 0.5922}]\n",
      "epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.00it/s, loss=0.409, metrics={'BinaryAUROC': 0.8042, 'f1': 0.5133, 'BinaryAveragePrecision': 0.5976}]\n",
      "epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.81it/s, loss=0.406, metrics={'BinaryAUROC': 0.8067, 'f1': 0.5159, 'BinaryAveragePrecision': 0.603}] \n",
      "epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.52it/s, loss=0.404, metrics={'BinaryAUROC': 0.8093, 'f1': 0.5232, 'BinaryAveragePrecision': 0.6097}]\n",
      "epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.37it/s, loss=0.403, metrics={'BinaryAUROC': 0.8109, 'f1': 0.5254, 'BinaryAveragePrecision': 0.61}]  \n",
      "epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.43it/s, loss=0.401, metrics={'BinaryAUROC': 0.8127, 'f1': 0.5306, 'BinaryAveragePrecision': 0.6142}]\n",
      "epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.57it/s, loss=0.399, metrics={'BinaryAUROC': 0.8155, 'f1': 0.5342, 'BinaryAveragePrecision': 0.6194}]\n",
      "epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.87it/s, loss=0.396, metrics={'BinaryAUROC': 0.8178, 'f1': 0.5428, 'BinaryAveragePrecision': 0.6282}]\n",
      "epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.40it/s, loss=0.394, metrics={'BinaryAUROC': 0.8199, 'f1': 0.542, 'BinaryAveragePrecision': 0.6278}] \n",
      "epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.78it/s, loss=0.391, metrics={'BinaryAUROC': 0.823, 'f1': 0.5471, 'BinaryAveragePrecision': 0.6393}] \n",
      "epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.18it/s, loss=0.392, metrics={'BinaryAUROC': 0.8226, 'f1': 0.5451, 'BinaryAveragePrecision': 0.6338}]\n",
      "epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.75it/s, loss=0.386, metrics={'BinaryAUROC': 0.8283, 'f1': 0.5493, 'BinaryAveragePrecision': 0.6442}]\n",
      "epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.14it/s, loss=0.385, metrics={'BinaryAUROC': 0.8303, 'f1': 0.547, 'BinaryAveragePrecision': 0.6474}] \n",
      "epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.63it/s, loss=0.383, metrics={'BinaryAUROC': 0.8321, 'f1': 0.56, 'BinaryAveragePrecision': 0.6495}]  \n",
      "epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.98it/s, loss=0.381, metrics={'BinaryAUROC': 0.8331, 'f1': 0.5544, 'BinaryAveragePrecision': 0.6543}]\n",
      "epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.02it/s, loss=0.381, metrics={'BinaryAUROC': 0.8327, 'f1': 0.5598, 'BinaryAveragePrecision': 0.6544}]\n",
      "epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.87it/s, loss=0.375, metrics={'BinaryAUROC': 0.8406, 'f1': 0.5694, 'BinaryAveragePrecision': 0.6642}]\n",
      "epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.34it/s, loss=0.371, metrics={'BinaryAUROC': 0.8435, 'f1': 0.5761, 'BinaryAveragePrecision': 0.6729}]\n",
      "epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.25it/s, loss=0.37, metrics={'BinaryAUROC': 0.8451, 'f1': 0.5747, 'BinaryAveragePrecision': 0.6708}] \n",
      "epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.25it/s, loss=0.37, metrics={'BinaryAUROC': 0.8446, 'f1': 0.5773, 'BinaryAveragePrecision': 0.6738}] \n",
      "epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.50it/s, loss=0.367, metrics={'BinaryAUROC': 0.8469, 'f1': 0.5858, 'BinaryAveragePrecision': 0.6788}]\n",
      "epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.02it/s, loss=0.364, metrics={'BinaryAUROC': 0.8504, 'f1': 0.5883, 'BinaryAveragePrecision': 0.6822}]\n",
      "epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.62it/s, loss=0.36, metrics={'BinaryAUROC': 0.8546, 'f1': 0.5893, 'BinaryAveragePrecision': 0.693}]  \n",
      "epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.59it/s, loss=0.36, metrics={'BinaryAUROC': 0.8523, 'f1': 0.5968, 'BinaryAveragePrecision': 0.6937}] \n",
      "epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.65it/s, loss=0.355, metrics={'BinaryAUROC': 0.8593, 'f1': 0.5933, 'BinaryAveragePrecision': 0.698}] \n",
      "epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.00it/s, loss=0.354, metrics={'BinaryAUROC': 0.8582, 'f1': 0.5945, 'BinaryAveragePrecision': 0.7012}]\n",
      "epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.93it/s, loss=0.352, metrics={'BinaryAUROC': 0.8608, 'f1': 0.6045, 'BinaryAveragePrecision': 0.7045}]\n",
      "epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.62it/s, loss=0.347, metrics={'BinaryAUROC': 0.8645, 'f1': 0.6109, 'BinaryAveragePrecision': 0.7123}]\n",
      "epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.87it/s, loss=0.346, metrics={'BinaryAUROC': 0.8649, 'f1': 0.6104, 'BinaryAveragePrecision': 0.7148}]\n",
      "epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.42it/s, loss=0.343, metrics={'BinaryAUROC': 0.8691, 'f1': 0.6121, 'BinaryAveragePrecision': 0.7196}]\n",
      "epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.56it/s, loss=0.343, metrics={'BinaryAUROC': 0.8685, 'f1': 0.6105, 'BinaryAveragePrecision': 0.7185}]\n",
      "epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.52it/s, loss=0.341, metrics={'BinaryAUROC': 0.8715, 'f1': 0.6182, 'BinaryAveragePrecision': 0.722}] \n",
      "epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.49it/s, loss=0.337, metrics={'BinaryAUROC': 0.8732, 'f1': 0.623, 'BinaryAveragePrecision': 0.7285}] \n",
      "epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.23it/s, loss=0.335, metrics={'BinaryAUROC': 0.8751, 'f1': 0.6254, 'BinaryAveragePrecision': 0.7324}]\n",
      "epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.40it/s, loss=0.334, metrics={'BinaryAUROC': 0.8748, 'f1': 0.6234, 'BinaryAveragePrecision': 0.7345}]\n",
      "epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.50it/s, loss=0.331, metrics={'BinaryAUROC': 0.8777, 'f1': 0.6318, 'BinaryAveragePrecision': 0.74}]  \n",
      "epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.34it/s, loss=0.328, metrics={'BinaryAUROC': 0.8819, 'f1': 0.6374, 'BinaryAveragePrecision': 0.7393}]\n",
      "epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.95it/s, loss=0.327, metrics={'BinaryAUROC': 0.8811, 'f1': 0.6401, 'BinaryAveragePrecision': 0.7438}]\n",
      "epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.58it/s, loss=0.323, metrics={'BinaryAUROC': 0.8841, 'f1': 0.643, 'BinaryAveragePrecision': 0.749}]  \n",
      "epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.76it/s, loss=0.327, metrics={'BinaryAUROC': 0.881, 'f1': 0.6377, 'BinaryAveragePrecision': 0.7441}] \n",
      "epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.30it/s, loss=0.319, metrics={'BinaryAUROC': 0.8872, 'f1': 0.6474, 'BinaryAveragePrecision': 0.7555}]\n",
      "epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.76it/s, loss=0.318, metrics={'BinaryAUROC': 0.8882, 'f1': 0.6529, 'BinaryAveragePrecision': 0.7562}]\n",
      "epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.26it/s, loss=0.317, metrics={'BinaryAUROC': 0.889, 'f1': 0.6515, 'BinaryAveragePrecision': 0.7575}] \n",
      "epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.72it/s, loss=0.321, metrics={'BinaryAUROC': 0.8859, 'f1': 0.643, 'BinaryAveragePrecision': 0.7503}] \n",
      "epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.61it/s, loss=0.318, metrics={'BinaryAUROC': 0.8876, 'f1': 0.6502, 'BinaryAveragePrecision': 0.7568}]\n",
      "epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.26it/s, loss=0.313, metrics={'BinaryAUROC': 0.8919, 'f1': 0.659, 'BinaryAveragePrecision': 0.7631}] \n",
      "epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.37it/s, loss=0.309, metrics={'BinaryAUROC': 0.8952, 'f1': 0.6571, 'BinaryAveragePrecision': 0.7696}]\n",
      "epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.23it/s, loss=0.307, metrics={'BinaryAUROC': 0.8967, 'f1': 0.6611, 'BinaryAveragePrecision': 0.7718}]\n",
      "epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.28it/s, loss=0.309, metrics={'BinaryAUROC': 0.8958, 'f1': 0.6632, 'BinaryAveragePrecision': 0.7693}]\n",
      "epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.18it/s, loss=0.306, metrics={'BinaryAUROC': 0.8963, 'f1': 0.6665, 'BinaryAveragePrecision': 0.7742}]\n",
      "epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.14it/s, loss=0.306, metrics={'BinaryAUROC': 0.8974, 'f1': 0.6674, 'BinaryAveragePrecision': 0.774}] \n",
      "epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.18it/s, loss=0.301, metrics={'BinaryAUROC': 0.9017, 'f1': 0.6685, 'BinaryAveragePrecision': 0.7803}]\n",
      "epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.07it/s, loss=0.302, metrics={'BinaryAUROC': 0.9003, 'f1': 0.6652, 'BinaryAveragePrecision': 0.7783}]\n",
      "epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.62it/s, loss=0.3, metrics={'BinaryAUROC': 0.902, 'f1': 0.6666, 'BinaryAveragePrecision': 0.7797}]   \n",
      "epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.55it/s, loss=0.298, metrics={'BinaryAUROC': 0.9034, 'f1': 0.665, 'BinaryAveragePrecision': 0.7843}] \n",
      "epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.87it/s, loss=0.298, metrics={'BinaryAUROC': 0.9021, 'f1': 0.6755, 'BinaryAveragePrecision': 0.784}] \n",
      "epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.93it/s, loss=0.294, metrics={'BinaryAUROC': 0.9059, 'f1': 0.6785, 'BinaryAveragePrecision': 0.7899}]\n",
      "epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.22it/s, loss=0.295, metrics={'BinaryAUROC': 0.9044, 'f1': 0.6799, 'BinaryAveragePrecision': 0.7881}]\n",
      "epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.84it/s, loss=0.291, metrics={'BinaryAUROC': 0.9082, 'f1': 0.6845, 'BinaryAveragePrecision': 0.7937}]\n",
      "epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.65it/s, loss=0.291, metrics={'BinaryAUROC': 0.9077, 'f1': 0.6851, 'BinaryAveragePrecision': 0.793}] \n",
      "epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.53it/s, loss=0.29, metrics={'BinaryAUROC': 0.9081, 'f1': 0.6878, 'BinaryAveragePrecision': 0.7967}] \n",
      "epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.40it/s, loss=0.289, metrics={'BinaryAUROC': 0.9093, 'f1': 0.6881, 'BinaryAveragePrecision': 0.7968}]\n",
      "epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.73it/s, loss=0.288, metrics={'BinaryAUROC': 0.9092, 'f1': 0.6872, 'BinaryAveragePrecision': 0.7973}]\n",
      "epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.89it/s, loss=0.289, metrics={'BinaryAUROC': 0.9086, 'f1': 0.6835, 'BinaryAveragePrecision': 0.7981}]\n",
      "epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.65it/s, loss=0.285, metrics={'BinaryAUROC': 0.9119, 'f1': 0.6899, 'BinaryAveragePrecision': 0.8016}]\n",
      "epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.35it/s, loss=0.282, metrics={'BinaryAUROC': 0.9128, 'f1': 0.6965, 'BinaryAveragePrecision': 0.8057}]\n",
      "epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.70it/s, loss=0.287, metrics={'BinaryAUROC': 0.9112, 'f1': 0.6896, 'BinaryAveragePrecision': 0.8008}]\n",
      "epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.45it/s, loss=0.278, metrics={'BinaryAUROC': 0.9153, 'f1': 0.6954, 'BinaryAveragePrecision': 0.81}]  \n",
      "epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.64it/s, loss=0.275, metrics={'BinaryAUROC': 0.9177, 'f1': 0.7033, 'BinaryAveragePrecision': 0.815}] \n",
      "epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.41it/s, loss=0.274, metrics={'BinaryAUROC': 0.9188, 'f1': 0.7076, 'BinaryAveragePrecision': 0.8163}]\n",
      "epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.94it/s, loss=0.272, metrics={'BinaryAUROC': 0.9192, 'f1': 0.7085, 'BinaryAveragePrecision': 0.8181}]\n",
      "epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.21it/s, loss=0.273, metrics={'BinaryAUROC': 0.9192, 'f1': 0.7056, 'BinaryAveragePrecision': 0.817}] \n",
      "epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.54it/s, loss=0.271, metrics={'BinaryAUROC': 0.9199, 'f1': 0.7091, 'BinaryAveragePrecision': 0.8187}]\n",
      "epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.36it/s, loss=0.276, metrics={'BinaryAUROC': 0.9183, 'f1': 0.6966, 'BinaryAveragePrecision': 0.8121}]\n",
      "epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.06it/s, loss=0.273, metrics={'BinaryAUROC': 0.92, 'f1': 0.7079, 'BinaryAveragePrecision': 0.8162}]  \n",
      "epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.95it/s, loss=0.271, metrics={'BinaryAUROC': 0.9197, 'f1': 0.7099, 'BinaryAveragePrecision': 0.8203}]\n",
      "epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.09it/s, loss=0.267, metrics={'BinaryAUROC': 0.9229, 'f1': 0.7076, 'BinaryAveragePrecision': 0.8229}]\n",
      "epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 35.52it/s, loss=0.267, metrics={'BinaryAUROC': 0.922, 'f1': 0.7134, 'BinaryAveragePrecision': 0.8231}] \n",
      "epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 34.91it/s, loss=0.267, metrics={'BinaryAUROC': 0.9231, 'f1': 0.7142, 'BinaryAveragePrecision': 0.8235}]\n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "trainer = Trainer(model, objective=\"binary\", accelerator=\"gpu\",\\\n",
    "                  metrics=[AUROC(task='binary'), F1Score, AveragePrecision(task='binary')])\n",
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 33.52it/s]\n",
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 34.31it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict on test\n",
    "X_wide_te = wide_preprocessor.transform(test_df)\n",
    "X_tab_te = tab_preprocessor.transform(test_df)\n",
    "preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)\n",
    "pred_probs = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.6923589337984494\n",
      "PrecisionRecall-AUC:0.4104363421893267\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "target = lukup[name]\n",
    "y = test_df[target].values\n",
    "print(\"ROC-AUC:{}\".format(roc_auc_score(y, pred_probs[:, 1])))\n",
    "print(\"PrecisionRecall-AUC:{}\".format(average_precision_score(y, pred_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Tab2Vec\n",
    "t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor)\n",
    "X_vec, y = t2v.transform(train_df, target_col=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4172393e-01, -2.9816014e-01,  2.1386049e+00, ...,\n",
       "         5.1322991e-01,  8.6423665e-01, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         5.5625874e-01,  4.9738172e-01,  8.4056890e-01],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         6.8739414e-01, -1.5589465e-01,  2.2773863e-01],\n",
       "       ...,\n",
       "       [-1.4172393e-01, -2.9816014e-01, -4.5036829e-01, ...,\n",
       "        -5.1062021e-02, -8.0248013e-02, -8.1868723e-02],\n",
       "       [-1.8721935e+00,  8.3828169e-01,  2.1386049e+00, ...,\n",
       "         3.1717189e-02,  1.0486166e-03, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "        -6.6634342e-02, -7.7423006e-02, -9.5329911e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_03_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
