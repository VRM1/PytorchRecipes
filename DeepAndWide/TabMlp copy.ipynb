{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabNet, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy, F1Score\n",
    "from pytorch_widedeep.datasets import load_adult\n",
    "import warnings\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning, message=\"unclosed.*<zmq.*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.134759</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.029047</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428605</td>\n",
       "      <td>-0.369043</td>\n",
       "      <td>-0.239816</td>\n",
       "      <td>0.503316</td>\n",
       "      <td>-0.039980</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>0.194622</td>\n",
       "      <td>0.536758</td>\n",
       "      <td>-0.180878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.483795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.357652</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>-0.525994</td>\n",
       "      <td>-0.512933</td>\n",
       "      <td>-0.070252</td>\n",
       "      <td>-0.126784</td>\n",
       "      <td>2.543032</td>\n",
       "      <td>0.228134</td>\n",
       "      <td>0.230763</td>\n",
       "      <td>0.436037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.674276</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.378129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.561493</td>\n",
       "      <td>0.571863</td>\n",
       "      <td>-0.160815</td>\n",
       "      <td>-0.083165</td>\n",
       "      <td>-0.154810</td>\n",
       "      <td>0.330267</td>\n",
       "      <td>-0.314136</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.751350</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.249166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>0.504203</td>\n",
       "      <td>0.545752</td>\n",
       "      <td>-0.148740</td>\n",
       "      <td>-0.109423</td>\n",
       "      <td>-0.132091</td>\n",
       "      <td>-0.132522</td>\n",
       "      <td>-0.107827</td>\n",
       "      <td>-0.141502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.365981</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126163</td>\n",
       "      <td>1.336657</td>\n",
       "      <td>1.378068</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>-0.061681</td>\n",
       "      <td>-0.041216</td>\n",
       "      <td>0.196217</td>\n",
       "      <td>-0.117776</td>\n",
       "      <td>-0.067081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE       AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0  -0.134759    0          1         1 -1.029047      2      1      1      1   \n",
       "1   1.483795    1          0         1  1.357652      2      1      1      1   \n",
       "2  -0.674276    1          0         1 -0.378129      0      1      1      1   \n",
       "3  -0.751350    0          2         0  1.249166      2      1      1      1   \n",
       "4  -0.365981    1          1         1  0.598248      2      1      1      1   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0      1  ...  -0.428605  -0.369043  -0.239816  0.503316 -0.039980 -0.012818   \n",
       "1      1  ...   0.579916  -0.525994  -0.512933 -0.070252 -0.126784  2.543032   \n",
       "2      1  ...   0.374450   0.561493   0.571863 -0.160815 -0.083165 -0.154810   \n",
       "3      1  ...   0.425778   0.504203   0.545752 -0.148740 -0.109423 -0.132091   \n",
       "4      1  ...   1.126163   1.336657   1.378068  0.020312 -0.061681 -0.041216   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0  0.194622  0.536758 -0.180878                           0  \n",
       "1  0.228134  0.230763  0.436037                           0  \n",
       "2  0.330267 -0.314136 -0.012122                           1  \n",
       "3 -0.132522 -0.107827 -0.141502                           0  \n",
       "4  0.196217 -0.117776 -0.067081                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lukup = {'defaultCredit':'default.payment.next.month', 'bank':'y'}\n",
    "name = 'defaultCredit'\n",
    "label = lukup[name]\n",
    "fold = 1\n",
    "train_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/train/data.csv'.format(name, fold))\n",
    "valid_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/valid/data.csv'.format(name, fold))\n",
    "test_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/test/data.csv'.format(name, fold))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'column set up'\n",
    "wide_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "\n",
    "cat_embed_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "continuous_cols = [\"LIMIT_BAL\", \"BILL_AMT1\", \"BILL_AMT1\", \"BILL_AMT2\", \\\n",
    "     \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", 'PAY_AMT1', 'PAY_AMT1',\\\n",
    "        'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "target = \"default.payment.next.month\"\n",
    "target = train_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "wide_preprocessor = WidePreprocessor(wide_cols=wide_cols)\n",
    "X_wide = wide_preprocessor.fit_transform(train_df)\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "    cat_embed_cols=cat_embed_cols, continuous_cols=continuous_cols  # type: ignore[arg-type]\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineeth/anaconda3/envs/pytorch_03_2023/lib/python3.10/site-packages/pytorch_widedeep/models/wide_deep.py:390: UserWarning: 'WideDeep' is a model comprised by multiple components and the 'deeptabular' component is 'TabNet'. We recommend using 'TabNet' in isolation. The reasons are: i)'TabNet' uses sparse regularization which partially losses its purpose when used in combination with other components. If you still want to use a multiple component model with 'TabNet', consider setting 'lambda_sparse' to 0 during training. ii) The feature importances will be computed only for TabNet but the model will comprise multiple components. Therefore, such importances will partially lose their 'meaning'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# build the model\n",
    "wide = Wide(input_dim=np.unique(X_wide).shape[0], pred_dim=1)\n",
    "tab_mlp = TabNet(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "    continuous_cols=continuous_cols,\n",
    "    dropout=0.5,\n",
    ")\n",
    "model = WideDeep(wide=wide, deeptabular=tab_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 32.23it/s, loss=0.839, metrics={'BinaryAUROC': 0.48, 'f1': 0.1944, 'BinaryAveragePrecision': 0.2196}]  \n",
      "epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.15it/s, loss=0.743, metrics={'BinaryAUROC': 0.5047, 'f1': 0.1838, 'BinaryAveragePrecision': 0.2328}]\n",
      "epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.26it/s, loss=0.664, metrics={'BinaryAUROC': 0.5469, 'f1': 0.2109, 'BinaryAveragePrecision': 0.2681}]\n",
      "epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.25it/s, loss=0.639, metrics={'BinaryAUROC': 0.562, 'f1': 0.2407, 'BinaryAveragePrecision': 0.2838}] \n",
      "epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.31it/s, loss=0.595, metrics={'BinaryAUROC': 0.5905, 'f1': 0.2642, 'BinaryAveragePrecision': 0.31}]  \n",
      "epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.10it/s, loss=0.571, metrics={'BinaryAUROC': 0.6126, 'f1': 0.2706, 'BinaryAveragePrecision': 0.3287}]\n",
      "epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.16it/s, loss=0.546, metrics={'BinaryAUROC': 0.6284, 'f1': 0.2918, 'BinaryAveragePrecision': 0.3542}]\n",
      "epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.16it/s, loss=0.534, metrics={'BinaryAUROC': 0.6451, 'f1': 0.3012, 'BinaryAveragePrecision': 0.3699}]\n",
      "epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.04it/s, loss=0.514, metrics={'BinaryAUROC': 0.662, 'f1': 0.3163, 'BinaryAveragePrecision': 0.3947}] \n",
      "epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.83it/s, loss=0.506, metrics={'BinaryAUROC': 0.6737, 'f1': 0.3288, 'BinaryAveragePrecision': 0.4033}]\n",
      "epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.70it/s, loss=0.496, metrics={'BinaryAUROC': 0.6861, 'f1': 0.3327, 'BinaryAveragePrecision': 0.4186}]\n",
      "epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.54it/s, loss=0.488, metrics={'BinaryAUROC': 0.6962, 'f1': 0.3453, 'BinaryAveragePrecision': 0.4311}]\n",
      "epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.48it/s, loss=0.483, metrics={'BinaryAUROC': 0.7011, 'f1': 0.3425, 'BinaryAveragePrecision': 0.4395}]\n",
      "epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.80it/s, loss=0.474, metrics={'BinaryAUROC': 0.7118, 'f1': 0.3528, 'BinaryAveragePrecision': 0.4596}]\n",
      "epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.79it/s, loss=0.471, metrics={'BinaryAUROC': 0.7168, 'f1': 0.3629, 'BinaryAveragePrecision': 0.4642}]\n",
      "epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.84it/s, loss=0.466, metrics={'BinaryAUROC': 0.7187, 'f1': 0.3688, 'BinaryAveragePrecision': 0.4733}]\n",
      "epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.78it/s, loss=0.462, metrics={'BinaryAUROC': 0.7249, 'f1': 0.3773, 'BinaryAveragePrecision': 0.481}] \n",
      "epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.20it/s, loss=0.46, metrics={'BinaryAUROC': 0.7296, 'f1': 0.3765, 'BinaryAveragePrecision': 0.4854}] \n",
      "epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.61it/s, loss=0.458, metrics={'BinaryAUROC': 0.7304, 'f1': 0.3858, 'BinaryAveragePrecision': 0.4903}]\n",
      "epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.33it/s, loss=0.456, metrics={'BinaryAUROC': 0.7328, 'f1': 0.3848, 'BinaryAveragePrecision': 0.4978}]\n",
      "epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.73it/s, loss=0.452, metrics={'BinaryAUROC': 0.7391, 'f1': 0.393, 'BinaryAveragePrecision': 0.5057}] \n",
      "epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.24it/s, loss=0.451, metrics={'BinaryAUROC': 0.7391, 'f1': 0.4, 'BinaryAveragePrecision': 0.5061}]   \n",
      "epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.58it/s, loss=0.452, metrics={'BinaryAUROC': 0.7404, 'f1': 0.3977, 'BinaryAveragePrecision': 0.5044}]\n",
      "epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.45it/s, loss=0.45, metrics={'BinaryAUROC': 0.7403, 'f1': 0.4037, 'BinaryAveragePrecision': 0.5067}] \n",
      "epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.24it/s, loss=0.449, metrics={'BinaryAUROC': 0.7424, 'f1': 0.4055, 'BinaryAveragePrecision': 0.5088}]\n",
      "epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.60it/s, loss=0.448, metrics={'BinaryAUROC': 0.7439, 'f1': 0.4098, 'BinaryAveragePrecision': 0.5145}]\n",
      "epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.45it/s, loss=0.446, metrics={'BinaryAUROC': 0.7485, 'f1': 0.4144, 'BinaryAveragePrecision': 0.5187}]\n",
      "epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.74it/s, loss=0.446, metrics={'BinaryAUROC': 0.7477, 'f1': 0.4205, 'BinaryAveragePrecision': 0.5175}]\n",
      "epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.63it/s, loss=0.445, metrics={'BinaryAUROC': 0.748, 'f1': 0.4281, 'BinaryAveragePrecision': 0.5215}] \n",
      "epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.80it/s, loss=0.444, metrics={'BinaryAUROC': 0.7497, 'f1': 0.4315, 'BinaryAveragePrecision': 0.5233}]\n",
      "epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.65it/s, loss=0.444, metrics={'BinaryAUROC': 0.7479, 'f1': 0.4313, 'BinaryAveragePrecision': 0.5262}]\n",
      "epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 43.93it/s, loss=0.444, metrics={'BinaryAUROC': 0.7484, 'f1': 0.4396, 'BinaryAveragePrecision': 0.5234}]\n",
      "epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.70it/s, loss=0.443, metrics={'BinaryAUROC': 0.7506, 'f1': 0.4494, 'BinaryAveragePrecision': 0.5277}]\n",
      "epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.70it/s, loss=0.443, metrics={'BinaryAUROC': 0.7499, 'f1': 0.4481, 'BinaryAveragePrecision': 0.5272}]\n",
      "epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.87it/s, loss=0.442, metrics={'BinaryAUROC': 0.7513, 'f1': 0.4526, 'BinaryAveragePrecision': 0.53}]  \n",
      "epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.43it/s, loss=0.442, metrics={'BinaryAUROC': 0.7514, 'f1': 0.4558, 'BinaryAveragePrecision': 0.5294}]\n",
      "epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.86it/s, loss=0.442, metrics={'BinaryAUROC': 0.7514, 'f1': 0.4579, 'BinaryAveragePrecision': 0.5304}]\n",
      "epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.90it/s, loss=0.442, metrics={'BinaryAUROC': 0.7533, 'f1': 0.4605, 'BinaryAveragePrecision': 0.5305}]\n",
      "epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 42.80it/s, loss=0.442, metrics={'BinaryAUROC': 0.7526, 'f1': 0.4624, 'BinaryAveragePrecision': 0.5272}]\n",
      "epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 43.82it/s, loss=0.441, metrics={'BinaryAUROC': 0.753, 'f1': 0.4632, 'BinaryAveragePrecision': 0.5322}] \n",
      "epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.93it/s, loss=0.441, metrics={'BinaryAUROC': 0.7518, 'f1': 0.4658, 'BinaryAveragePrecision': 0.5302}]\n",
      "epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.20it/s, loss=0.441, metrics={'BinaryAUROC': 0.7531, 'f1': 0.4683, 'BinaryAveragePrecision': 0.5312}]\n",
      "epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 36.53it/s, loss=0.441, metrics={'BinaryAUROC': 0.7523, 'f1': 0.4676, 'BinaryAveragePrecision': 0.5309}]\n",
      "epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.18it/s, loss=0.441, metrics={'BinaryAUROC': 0.7539, 'f1': 0.4709, 'BinaryAveragePrecision': 0.5332}]\n",
      "epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.19it/s, loss=0.441, metrics={'BinaryAUROC': 0.7534, 'f1': 0.4702, 'BinaryAveragePrecision': 0.5322}]\n",
      "epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.77it/s, loss=0.44, metrics={'BinaryAUROC': 0.7547, 'f1': 0.4701, 'BinaryAveragePrecision': 0.5327}] \n",
      "epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.56it/s, loss=0.44, metrics={'BinaryAUROC': 0.7547, 'f1': 0.4697, 'BinaryAveragePrecision': 0.5347}] \n",
      "epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.72it/s, loss=0.44, metrics={'BinaryAUROC': 0.7544, 'f1': 0.4709, 'BinaryAveragePrecision': 0.5332}] \n",
      "epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.68it/s, loss=0.44, metrics={'BinaryAUROC': 0.7538, 'f1': 0.4741, 'BinaryAveragePrecision': 0.5346}] \n",
      "epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.92it/s, loss=0.44, metrics={'BinaryAUROC': 0.7545, 'f1': 0.473, 'BinaryAveragePrecision': 0.534}]   \n",
      "epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.26it/s, loss=0.44, metrics={'BinaryAUROC': 0.7546, 'f1': 0.4748, 'BinaryAveragePrecision': 0.5347}] \n",
      "epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.45it/s, loss=0.44, metrics={'BinaryAUROC': 0.7546, 'f1': 0.4731, 'BinaryAveragePrecision': 0.5364}] \n",
      "epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.14it/s, loss=0.44, metrics={'BinaryAUROC': 0.7538, 'f1': 0.4736, 'BinaryAveragePrecision': 0.5333}] \n",
      "epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.72it/s, loss=0.44, metrics={'BinaryAUROC': 0.7551, 'f1': 0.4735, 'BinaryAveragePrecision': 0.5345}] \n",
      "epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.79it/s, loss=0.44, metrics={'BinaryAUROC': 0.7542, 'f1': 0.4746, 'BinaryAveragePrecision': 0.5355}] \n",
      "epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.91it/s, loss=0.44, metrics={'BinaryAUROC': 0.7543, 'f1': 0.4751, 'BinaryAveragePrecision': 0.535}]  \n",
      "epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.61it/s, loss=0.44, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4757, 'BinaryAveragePrecision': 0.5352}] \n",
      "epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.49it/s, loss=0.44, metrics={'BinaryAUROC': 0.7555, 'f1': 0.4752, 'BinaryAveragePrecision': 0.5353}] \n",
      "epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.25it/s, loss=0.439, metrics={'BinaryAUROC': 0.7555, 'f1': 0.4771, 'BinaryAveragePrecision': 0.537}] \n",
      "epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.82it/s, loss=0.439, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4752, 'BinaryAveragePrecision': 0.5365}]\n",
      "epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.91it/s, loss=0.44, metrics={'BinaryAUROC': 0.7546, 'f1': 0.4742, 'BinaryAveragePrecision': 0.5352}] \n",
      "epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.35it/s, loss=0.439, metrics={'BinaryAUROC': 0.7554, 'f1': 0.4755, 'BinaryAveragePrecision': 0.5369}]\n",
      "epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.61it/s, loss=0.439, metrics={'BinaryAUROC': 0.7544, 'f1': 0.475, 'BinaryAveragePrecision': 0.5369}] \n",
      "epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.32it/s, loss=0.44, metrics={'BinaryAUROC': 0.7544, 'f1': 0.4769, 'BinaryAveragePrecision': 0.5357}] \n",
      "epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.12it/s, loss=0.439, metrics={'BinaryAUROC': 0.7563, 'f1': 0.4773, 'BinaryAveragePrecision': 0.5366}]\n",
      "epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.31it/s, loss=0.439, metrics={'BinaryAUROC': 0.7549, 'f1': 0.475, 'BinaryAveragePrecision': 0.536}]  \n",
      "epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.31it/s, loss=0.439, metrics={'BinaryAUROC': 0.7551, 'f1': 0.4761, 'BinaryAveragePrecision': 0.5368}]\n",
      "epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.12it/s, loss=0.44, metrics={'BinaryAUROC': 0.7557, 'f1': 0.4762, 'BinaryAveragePrecision': 0.5347}] \n",
      "epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.27it/s, loss=0.439, metrics={'BinaryAUROC': 0.7555, 'f1': 0.4764, 'BinaryAveragePrecision': 0.5364}]\n",
      "epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.78it/s, loss=0.44, metrics={'BinaryAUROC': 0.7547, 'f1': 0.4778, 'BinaryAveragePrecision': 0.5365}] \n",
      "epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.57it/s, loss=0.439, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4773, 'BinaryAveragePrecision': 0.5359}]\n",
      "epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.83it/s, loss=0.439, metrics={'BinaryAUROC': 0.7556, 'f1': 0.4753, 'BinaryAveragePrecision': 0.5375}]\n",
      "epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.74it/s, loss=0.439, metrics={'BinaryAUROC': 0.7557, 'f1': 0.4767, 'BinaryAveragePrecision': 0.5373}]\n",
      "epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.40it/s, loss=0.439, metrics={'BinaryAUROC': 0.7554, 'f1': 0.4757, 'BinaryAveragePrecision': 0.5374}]\n",
      "epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.95it/s, loss=0.439, metrics={'BinaryAUROC': 0.7556, 'f1': 0.4773, 'BinaryAveragePrecision': 0.5363}]\n",
      "epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.39it/s, loss=0.439, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4769, 'BinaryAveragePrecision': 0.5376}]\n",
      "epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.11it/s, loss=0.439, metrics={'BinaryAUROC': 0.7558, 'f1': 0.4755, 'BinaryAveragePrecision': 0.5378}]\n",
      "epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.05it/s, loss=0.439, metrics={'BinaryAUROC': 0.7554, 'f1': 0.4767, 'BinaryAveragePrecision': 0.5378}]\n",
      "epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.11it/s, loss=0.439, metrics={'BinaryAUROC': 0.7554, 'f1': 0.4766, 'BinaryAveragePrecision': 0.5375}]\n",
      "epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.01it/s, loss=0.439, metrics={'BinaryAUROC': 0.7556, 'f1': 0.4757, 'BinaryAveragePrecision': 0.5381}]\n",
      "epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.48it/s, loss=0.439, metrics={'BinaryAUROC': 0.7557, 'f1': 0.4762, 'BinaryAveragePrecision': 0.5377}]\n",
      "epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.20it/s, loss=0.439, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4767, 'BinaryAveragePrecision': 0.5381}]\n",
      "epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.90it/s, loss=0.439, metrics={'BinaryAUROC': 0.7559, 'f1': 0.4765, 'BinaryAveragePrecision': 0.5374}]\n",
      "epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.38it/s, loss=0.439, metrics={'BinaryAUROC': 0.7555, 'f1': 0.4777, 'BinaryAveragePrecision': 0.5374}]\n",
      "epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.04it/s, loss=0.439, metrics={'BinaryAUROC': 0.7566, 'f1': 0.4767, 'BinaryAveragePrecision': 0.5381}]\n",
      "epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.78it/s, loss=0.439, metrics={'BinaryAUROC': 0.7553, 'f1': 0.4772, 'BinaryAveragePrecision': 0.5378}]\n",
      "epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.32it/s, loss=0.439, metrics={'BinaryAUROC': 0.7558, 'f1': 0.4763, 'BinaryAveragePrecision': 0.5375}]\n",
      "epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.30it/s, loss=0.439, metrics={'BinaryAUROC': 0.7561, 'f1': 0.4766, 'BinaryAveragePrecision': 0.5379}]\n",
      "epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.07it/s, loss=0.439, metrics={'BinaryAUROC': 0.7562, 'f1': 0.4763, 'BinaryAveragePrecision': 0.5385}]\n",
      "epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 41.00it/s, loss=0.439, metrics={'BinaryAUROC': 0.7559, 'f1': 0.4759, 'BinaryAveragePrecision': 0.5381}]\n",
      "epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.70it/s, loss=0.439, metrics={'BinaryAUROC': 0.7559, 'f1': 0.4763, 'BinaryAveragePrecision': 0.5379}]\n",
      "epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.72it/s, loss=0.439, metrics={'BinaryAUROC': 0.7552, 'f1': 0.4761, 'BinaryAveragePrecision': 0.5373}]\n",
      "epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.79it/s, loss=0.439, metrics={'BinaryAUROC': 0.7555, 'f1': 0.4769, 'BinaryAveragePrecision': 0.5379}]\n",
      "epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 42.11it/s, loss=0.439, metrics={'BinaryAUROC': 0.7557, 'f1': 0.4769, 'BinaryAveragePrecision': 0.5386}]\n",
      "epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.45it/s, loss=0.439, metrics={'BinaryAUROC': 0.7558, 'f1': 0.4768, 'BinaryAveragePrecision': 0.5377}]\n",
      "epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.22it/s, loss=0.439, metrics={'BinaryAUROC': 0.756, 'f1': 0.4759, 'BinaryAveragePrecision': 0.5383}] \n",
      "epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.75it/s, loss=0.439, metrics={'BinaryAUROC': 0.757, 'f1': 0.4756, 'BinaryAveragePrecision': 0.5396}] \n",
      "epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 39.99it/s, loss=0.439, metrics={'BinaryAUROC': 0.7557, 'f1': 0.4755, 'BinaryAveragePrecision': 0.5383}]\n",
      "epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.51it/s, loss=0.439, metrics={'BinaryAUROC': 0.7561, 'f1': 0.4766, 'BinaryAveragePrecision': 0.5384}]\n",
      "epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:02<00:00, 40.39it/s, loss=0.439, metrics={'BinaryAUROC': 0.756, 'f1': 0.4773, 'BinaryAveragePrecision': 0.5388}] \n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "trainer = Trainer(model, objective=\"binary\", accelerator=\"gpu\",\\\n",
    "                  metrics=[AUROC(task='binary'), F1Score, AveragePrecision(task='binary')])\n",
    "trainer.fit(\n",
    "    X_wide=X_wide,\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 43.22it/s]\n",
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 44.99it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict on test\n",
    "X_wide_te = wide_preprocessor.transform(test_df)\n",
    "X_tab_te = tab_preprocessor.transform(test_df)\n",
    "preds = trainer.predict(X_wide=X_wide_te, X_tab=X_tab_te)\n",
    "pred_probs = trainer.predict_proba(X_wide=X_wide_te, X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.7512364801214417\n",
      "PrecisionRecall-AUC:0.5330399295236273\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "target = lukup[name]\n",
    "y = test_df[target].values\n",
    "print(\"ROC-AUC:{}\".format(roc_auc_score(y, pred_probs[:, 1])))\n",
    "print(\"PrecisionRecall-AUC:{}\".format(average_precision_score(y, pred_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Tab2Vec\n",
    "t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor)\n",
    "X_vec, y = t2v.transform(train_df, target_col=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4172393e-01, -2.9816014e-01,  2.1386049e+00, ...,\n",
       "         5.1322991e-01,  8.6423665e-01, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         5.5625874e-01,  4.9738172e-01,  8.4056890e-01],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         6.8739414e-01, -1.5589465e-01,  2.2773863e-01],\n",
       "       ...,\n",
       "       [-1.4172393e-01, -2.9816014e-01, -4.5036829e-01, ...,\n",
       "        -5.1062021e-02, -8.0248013e-02, -8.1868723e-02],\n",
       "       [-1.8721935e+00,  8.3828169e-01,  2.1386049e+00, ...,\n",
       "         3.1717189e-02,  1.0486166e-03, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "        -6.6634342e-02, -7.7423006e-02, -9.5329911e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_03_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
