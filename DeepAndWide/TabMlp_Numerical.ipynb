{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vineeth/anaconda3/envs/pytorch_03_2023/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.preprocessing import WidePreprocessor, TabPreprocessor\n",
    "from pytorch_widedeep.models import Wide, TabMlp, WideDeep\n",
    "from pytorch_widedeep.metrics import Accuracy, F1Score\n",
    "from pytorch_widedeep.datasets import load_adult\n",
    "import warnings\n",
    "from torchmetrics import AveragePrecision, AUROC\n",
    "warnings.filterwarnings(\"ignore\", category=ResourceWarning, message=\"unclosed.*<zmq.*>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EDUCATION</th>\n",
       "      <th>MARRIAGE</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PAY_0</th>\n",
       "      <th>PAY_2</th>\n",
       "      <th>PAY_3</th>\n",
       "      <th>PAY_4</th>\n",
       "      <th>PAY_5</th>\n",
       "      <th>...</th>\n",
       "      <th>BILL_AMT4</th>\n",
       "      <th>BILL_AMT5</th>\n",
       "      <th>BILL_AMT6</th>\n",
       "      <th>PAY_AMT1</th>\n",
       "      <th>PAY_AMT2</th>\n",
       "      <th>PAY_AMT3</th>\n",
       "      <th>PAY_AMT4</th>\n",
       "      <th>PAY_AMT5</th>\n",
       "      <th>PAY_AMT6</th>\n",
       "      <th>default.payment.next.month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.134759</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.029047</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.428605</td>\n",
       "      <td>-0.369043</td>\n",
       "      <td>-0.239816</td>\n",
       "      <td>0.503316</td>\n",
       "      <td>-0.039980</td>\n",
       "      <td>-0.012818</td>\n",
       "      <td>0.194622</td>\n",
       "      <td>0.536758</td>\n",
       "      <td>-0.180878</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.483795</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.357652</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.579916</td>\n",
       "      <td>-0.525994</td>\n",
       "      <td>-0.512933</td>\n",
       "      <td>-0.070252</td>\n",
       "      <td>-0.126784</td>\n",
       "      <td>2.543032</td>\n",
       "      <td>0.228134</td>\n",
       "      <td>0.230763</td>\n",
       "      <td>0.436037</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.674276</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.378129</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.374450</td>\n",
       "      <td>0.561493</td>\n",
       "      <td>0.571863</td>\n",
       "      <td>-0.160815</td>\n",
       "      <td>-0.083165</td>\n",
       "      <td>-0.154810</td>\n",
       "      <td>0.330267</td>\n",
       "      <td>-0.314136</td>\n",
       "      <td>-0.012122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.751350</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1.249166</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.425778</td>\n",
       "      <td>0.504203</td>\n",
       "      <td>0.545752</td>\n",
       "      <td>-0.148740</td>\n",
       "      <td>-0.109423</td>\n",
       "      <td>-0.132091</td>\n",
       "      <td>-0.132522</td>\n",
       "      <td>-0.107827</td>\n",
       "      <td>-0.141502</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.365981</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.598248</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.126163</td>\n",
       "      <td>1.336657</td>\n",
       "      <td>1.378068</td>\n",
       "      <td>0.020312</td>\n",
       "      <td>-0.061681</td>\n",
       "      <td>-0.041216</td>\n",
       "      <td>0.196217</td>\n",
       "      <td>-0.117776</td>\n",
       "      <td>-0.067081</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   LIMIT_BAL  SEX  EDUCATION  MARRIAGE       AGE  PAY_0  PAY_2  PAY_3  PAY_4  \\\n",
       "0  -0.134759    0          1         1 -1.029047      2      1      1      1   \n",
       "1   1.483795    1          0         1  1.357652      2      1      1      1   \n",
       "2  -0.674276    1          0         1 -0.378129      0      1      1      1   \n",
       "3  -0.751350    0          2         0  1.249166      2      1      1      1   \n",
       "4  -0.365981    1          1         1  0.598248      2      1      1      1   \n",
       "\n",
       "   PAY_5  ...  BILL_AMT4  BILL_AMT5  BILL_AMT6  PAY_AMT1  PAY_AMT2  PAY_AMT3  \\\n",
       "0      1  ...  -0.428605  -0.369043  -0.239816  0.503316 -0.039980 -0.012818   \n",
       "1      1  ...   0.579916  -0.525994  -0.512933 -0.070252 -0.126784  2.543032   \n",
       "2      1  ...   0.374450   0.561493   0.571863 -0.160815 -0.083165 -0.154810   \n",
       "3      1  ...   0.425778   0.504203   0.545752 -0.148740 -0.109423 -0.132091   \n",
       "4      1  ...   1.126163   1.336657   1.378068  0.020312 -0.061681 -0.041216   \n",
       "\n",
       "   PAY_AMT4  PAY_AMT5  PAY_AMT6  default.payment.next.month  \n",
       "0  0.194622  0.536758 -0.180878                           0  \n",
       "1  0.228134  0.230763  0.436037                           0  \n",
       "2  0.330267 -0.314136 -0.012122                           1  \n",
       "3 -0.132522 -0.107827 -0.141502                           0  \n",
       "4  0.196217 -0.117776 -0.067081                           0  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lukup = {'defaultCredit':'default.payment.next.month', 'bank':'y'}\n",
    "name = 'defaultCredit'\n",
    "label = lukup[name]\n",
    "fold = 1\n",
    "train_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/train/data.csv'.format(name, fold))\n",
    "valid_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/valid/data.csv'.format(name, fold))\n",
    "test_df = pd.read_csv('/home/vineeth/Documents/GitWorkSpace/PytorchRecipes/SimpleMLP/Dataset/{}/fold{}/test/data.csv'.format(name, fold))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the 'column set up'\n",
    "wide_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "\n",
    "cat_embed_cols = [\n",
    "    \"SEX\",\n",
    "    \"EDUCATION\",\n",
    "    \"MARRIAGE\",\n",
    "    \"PAY_0\",\n",
    "    \"PAY_2\",\n",
    "    \"PAY_3\",\n",
    "    \"PAY_4\",\n",
    "    \"PAY_5\",\n",
    "    \"PAY_6\"\n",
    "]\n",
    "continuous_cols = [\"LIMIT_BAL\", \"BILL_AMT1\", \"BILL_AMT1\", \"BILL_AMT2\", \\\n",
    "     \"BILL_AMT3\", \"BILL_AMT4\", \"BILL_AMT5\", \"BILL_AMT6\", 'PAY_AMT1', 'PAY_AMT1',\\\n",
    "        'PAY_AMT2', 'PAY_AMT3', 'PAY_AMT4', 'PAY_AMT5', 'PAY_AMT6']\n",
    "target = \"default.payment.next.month\"\n",
    "target = train_df[target].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "tab_preprocessor = TabPreprocessor(\n",
    " continuous_cols=continuous_cols  # type: ignore[arg-type]\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model\n",
    "tab_mlp = TabMlp(\n",
    "    column_idx=tab_preprocessor.column_idx,\n",
    "    continuous_cols=continuous_cols,\n",
    "    mlp_hidden_dims=[400, 200],\n",
    "    mlp_dropout=0.5,\n",
    "    mlp_activation=\"leaky_relu\"\n",
    ")\n",
    "model = WideDeep(deeptabular=tab_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:01<00:00, 64.28it/s, loss=0.543, metrics={'BinaryAUROC': 0.551, 'f1': 0.0079, 'BinaryAveragePrecision': 0.2419}]  \n",
      "epoch 2: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.75it/s, loss=0.52, metrics={'BinaryAUROC': 0.5979, 'f1': 0.0, 'BinaryAveragePrecision': 0.2834}]  \n",
      "epoch 3: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.52it/s, loss=0.52, metrics={'BinaryAUROC': 0.6, 'f1': 0.0013, 'BinaryAveragePrecision': 0.2806}]    \n",
      "epoch 4: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 109.62it/s, loss=0.516, metrics={'BinaryAUROC': 0.6103, 'f1': 0.0013, 'BinaryAveragePrecision': 0.2873}]\n",
      "epoch 5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.73it/s, loss=0.519, metrics={'BinaryAUROC': 0.603, 'f1': 0.0, 'BinaryAveragePrecision': 0.282}]  \n",
      "epoch 6: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 107.13it/s, loss=0.515, metrics={'BinaryAUROC': 0.6095, 'f1': 0.0, 'BinaryAveragePrecision': 0.2859}]\n",
      "epoch 7: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.67it/s, loss=0.515, metrics={'BinaryAUROC': 0.6114, 'f1': 0.0004, 'BinaryAveragePrecision': 0.2893}]\n",
      "epoch 8: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.96it/s, loss=0.514, metrics={'BinaryAUROC': 0.616, 'f1': 0.0, 'BinaryAveragePrecision': 0.2949}] \n",
      "epoch 9: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.27it/s, loss=0.513, metrics={'BinaryAUROC': 0.6208, 'f1': 0.0, 'BinaryAveragePrecision': 0.2992}]\n",
      "epoch 10: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.54it/s, loss=0.513, metrics={'BinaryAUROC': 0.6194, 'f1': 0.0, 'BinaryAveragePrecision': 0.2997}]\n",
      "epoch 11: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 106.94it/s, loss=0.512, metrics={'BinaryAUROC': 0.6237, 'f1': 0.0, 'BinaryAveragePrecision': 0.2996}]\n",
      "epoch 12: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 110.10it/s, loss=0.512, metrics={'BinaryAUROC': 0.622, 'f1': 0.0, 'BinaryAveragePrecision': 0.2976}] \n",
      "epoch 13: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.58it/s, loss=0.51, metrics={'BinaryAUROC': 0.6293, 'f1': 0.0, 'BinaryAveragePrecision': 0.3079}] \n",
      "epoch 14: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.14it/s, loss=0.51, metrics={'BinaryAUROC': 0.6285, 'f1': 0.0004, 'BinaryAveragePrecision': 0.3078}] \n",
      "epoch 15: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.70it/s, loss=0.513, metrics={'BinaryAUROC': 0.6164, 'f1': 0.0, 'BinaryAveragePrecision': 0.2936}]\n",
      "epoch 16: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.79it/s, loss=0.51, metrics={'BinaryAUROC': 0.6299, 'f1': 0.0, 'BinaryAveragePrecision': 0.3101}] \n",
      "epoch 17: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.98it/s, loss=0.511, metrics={'BinaryAUROC': 0.6248, 'f1': 0.0, 'BinaryAveragePrecision': 0.3032}]\n",
      "epoch 18: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.97it/s, loss=0.51, metrics={'BinaryAUROC': 0.627, 'f1': 0.0004, 'BinaryAveragePrecision': 0.3059}]  \n",
      "epoch 19: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 108.17it/s, loss=0.51, metrics={'BinaryAUROC': 0.6275, 'f1': 0.0, 'BinaryAveragePrecision': 0.3031}] \n",
      "epoch 20: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.31it/s, loss=0.508, metrics={'BinaryAUROC': 0.6351, 'f1': 0.0, 'BinaryAveragePrecision': 0.3142}] \n",
      "epoch 21: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.63it/s, loss=0.509, metrics={'BinaryAUROC': 0.6283, 'f1': 0.0021, 'BinaryAveragePrecision': 0.3032}]\n",
      "epoch 22: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.71it/s, loss=0.51, metrics={'BinaryAUROC': 0.6286, 'f1': 0.0004, 'BinaryAveragePrecision': 0.3057}] \n",
      "epoch 23: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.22it/s, loss=0.51, metrics={'BinaryAUROC': 0.6309, 'f1': 0.0, 'BinaryAveragePrecision': 0.3047}] \n",
      "epoch 24: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.02it/s, loss=0.51, metrics={'BinaryAUROC': 0.627, 'f1': 0.0, 'BinaryAveragePrecision': 0.3065}]  \n",
      "epoch 25: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.27it/s, loss=0.509, metrics={'BinaryAUROC': 0.631, 'f1': 0.0004, 'BinaryAveragePrecision': 0.3113}] \n",
      "epoch 26: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 96.78it/s, loss=0.51, metrics={'BinaryAUROC': 0.6289, 'f1': 0.0021, 'BinaryAveragePrecision': 0.3059}]  \n",
      "epoch 27: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.46it/s, loss=0.508, metrics={'BinaryAUROC': 0.6334, 'f1': 0.0017, 'BinaryAveragePrecision': 0.3146}]\n",
      "epoch 28: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.64it/s, loss=0.509, metrics={'BinaryAUROC': 0.6327, 'f1': 0.0029, 'BinaryAveragePrecision': 0.3113}]\n",
      "epoch 29: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.68it/s, loss=0.508, metrics={'BinaryAUROC': 0.6367, 'f1': 0.0021, 'BinaryAveragePrecision': 0.3135}]\n",
      "epoch 30: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.10it/s, loss=0.509, metrics={'BinaryAUROC': 0.6324, 'f1': 0.0017, 'BinaryAveragePrecision': 0.3055}]\n",
      "epoch 31: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 107.39it/s, loss=0.507, metrics={'BinaryAUROC': 0.6352, 'f1': 0.0013, 'BinaryAveragePrecision': 0.3079}]\n",
      "epoch 32: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.50it/s, loss=0.508, metrics={'BinaryAUROC': 0.6332, 'f1': 0.0025, 'BinaryAveragePrecision': 0.3099}]\n",
      "epoch 33: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.24it/s, loss=0.508, metrics={'BinaryAUROC': 0.637, 'f1': 0.0017, 'BinaryAveragePrecision': 0.3144}] \n",
      "epoch 34: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.09it/s, loss=0.508, metrics={'BinaryAUROC': 0.6341, 'f1': 0.0013, 'BinaryAveragePrecision': 0.3114}]\n",
      "epoch 35: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.90it/s, loss=0.509, metrics={'BinaryAUROC': 0.6321, 'f1': 0.0013, 'BinaryAveragePrecision': 0.3098}]\n",
      "epoch 36: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.29it/s, loss=0.51, metrics={'BinaryAUROC': 0.6277, 'f1': 0.0013, 'BinaryAveragePrecision': 0.3053}] \n",
      "epoch 37: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.29it/s, loss=0.508, metrics={'BinaryAUROC': 0.6337, 'f1': 0.0046, 'BinaryAveragePrecision': 0.3117}]\n",
      "epoch 38: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.03it/s, loss=0.508, metrics={'BinaryAUROC': 0.6349, 'f1': 0.0042, 'BinaryAveragePrecision': 0.3157}]\n",
      "epoch 39: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.90it/s, loss=0.507, metrics={'BinaryAUROC': 0.6394, 'f1': 0.0029, 'BinaryAveragePrecision': 0.3175}]\n",
      "epoch 40: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.45it/s, loss=0.507, metrics={'BinaryAUROC': 0.638, 'f1': 0.0034, 'BinaryAveragePrecision': 0.3105}] \n",
      "epoch 41: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 106.46it/s, loss=0.506, metrics={'BinaryAUROC': 0.643, 'f1': 0.0008, 'BinaryAveragePrecision': 0.3179}] \n",
      "epoch 42: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.10it/s, loss=0.507, metrics={'BinaryAUROC': 0.639, 'f1': 0.0038, 'BinaryAveragePrecision': 0.3133}] \n",
      "epoch 43: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.40it/s, loss=0.506, metrics={'BinaryAUROC': 0.6407, 'f1': 0.0004, 'BinaryAveragePrecision': 0.3189}]\n",
      "epoch 44: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.45it/s, loss=0.508, metrics={'BinaryAUROC': 0.6345, 'f1': 0.0017, 'BinaryAveragePrecision': 0.3133}]\n",
      "epoch 45: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.60it/s, loss=0.505, metrics={'BinaryAUROC': 0.6437, 'f1': 0.0029, 'BinaryAveragePrecision': 0.3233}]\n",
      "epoch 46: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.76it/s, loss=0.506, metrics={'BinaryAUROC': 0.641, 'f1': 0.0042, 'BinaryAveragePrecision': 0.3177}] \n",
      "epoch 47: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.71it/s, loss=0.506, metrics={'BinaryAUROC': 0.6397, 'f1': 0.0038, 'BinaryAveragePrecision': 0.3213}]\n",
      "epoch 48: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.88it/s, loss=0.505, metrics={'BinaryAUROC': 0.6437, 'f1': 0.0046, 'BinaryAveragePrecision': 0.3229}]\n",
      "epoch 49: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.57it/s, loss=0.506, metrics={'BinaryAUROC': 0.6417, 'f1': 0.005, 'BinaryAveragePrecision': 0.3203}] \n",
      "epoch 50: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.85it/s, loss=0.508, metrics={'BinaryAUROC': 0.6389, 'f1': 0.0075, 'BinaryAveragePrecision': 0.3151}]\n",
      "epoch 51: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.12it/s, loss=0.507, metrics={'BinaryAUROC': 0.6391, 'f1': 0.0046, 'BinaryAveragePrecision': 0.3181}]\n",
      "epoch 52: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.06it/s, loss=0.505, metrics={'BinaryAUROC': 0.6446, 'f1': 0.0079, 'BinaryAveragePrecision': 0.3258}]\n",
      "epoch 53: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 98.34it/s, loss=0.505, metrics={'BinaryAUROC': 0.6434, 'f1': 0.0092, 'BinaryAveragePrecision': 0.3218}] \n",
      "epoch 54: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.79it/s, loss=0.507, metrics={'BinaryAUROC': 0.6397, 'f1': 0.0087, 'BinaryAveragePrecision': 0.3192}]\n",
      "epoch 55: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 107.47it/s, loss=0.506, metrics={'BinaryAUROC': 0.6402, 'f1': 0.0058, 'BinaryAveragePrecision': 0.3177}]\n",
      "epoch 56: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.75it/s, loss=0.507, metrics={'BinaryAUROC': 0.6385, 'f1': 0.005, 'BinaryAveragePrecision': 0.3144}]  \n",
      "epoch 57: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 97.05it/s, loss=0.505, metrics={'BinaryAUROC': 0.6398, 'f1': 0.0063, 'BinaryAveragePrecision': 0.3203}] \n",
      "epoch 58: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.15it/s, loss=0.505, metrics={'BinaryAUROC': 0.6436, 'f1': 0.005, 'BinaryAveragePrecision': 0.3215}] \n",
      "epoch 59: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.79it/s, loss=0.504, metrics={'BinaryAUROC': 0.6469, 'f1': 0.0083, 'BinaryAveragePrecision': 0.3243}] \n",
      "epoch 60: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.73it/s, loss=0.506, metrics={'BinaryAUROC': 0.6426, 'f1': 0.0058, 'BinaryAveragePrecision': 0.3182}] \n",
      "epoch 61: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.41it/s, loss=0.506, metrics={'BinaryAUROC': 0.642, 'f1': 0.0079, 'BinaryAveragePrecision': 0.3221}] \n",
      "epoch 62: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 97.37it/s, loss=0.507, metrics={'BinaryAUROC': 0.6393, 'f1': 0.0075, 'BinaryAveragePrecision': 0.3159}] \n",
      "epoch 63: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.05it/s, loss=0.505, metrics={'BinaryAUROC': 0.6468, 'f1': 0.0083, 'BinaryAveragePrecision': 0.3253}]\n",
      "epoch 64: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.77it/s, loss=0.506, metrics={'BinaryAUROC': 0.6404, 'f1': 0.0158, 'BinaryAveragePrecision': 0.3243}] \n",
      "epoch 65: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.10it/s, loss=0.506, metrics={'BinaryAUROC': 0.6407, 'f1': 0.0145, 'BinaryAveragePrecision': 0.3211}]\n",
      "epoch 66: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 107.14it/s, loss=0.505, metrics={'BinaryAUROC': 0.6464, 'f1': 0.01, 'BinaryAveragePrecision': 0.3243}]  \n",
      "epoch 67: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.22it/s, loss=0.506, metrics={'BinaryAUROC': 0.6431, 'f1': 0.0133, 'BinaryAveragePrecision': 0.3226}]\n",
      "epoch 68: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 107.97it/s, loss=0.506, metrics={'BinaryAUROC': 0.6431, 'f1': 0.0087, 'BinaryAveragePrecision': 0.3193}]\n",
      "epoch 69: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.11it/s, loss=0.506, metrics={'BinaryAUROC': 0.6407, 'f1': 0.0112, 'BinaryAveragePrecision': 0.3181}]\n",
      "epoch 70: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.39it/s, loss=0.505, metrics={'BinaryAUROC': 0.6428, 'f1': 0.0116, 'BinaryAveragePrecision': 0.325}] \n",
      "epoch 71: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.39it/s, loss=0.506, metrics={'BinaryAUROC': 0.6404, 'f1': 0.01, 'BinaryAveragePrecision': 0.3207}]  \n",
      "epoch 72: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.91it/s, loss=0.505, metrics={'BinaryAUROC': 0.6436, 'f1': 0.0121, 'BinaryAveragePrecision': 0.3234}]\n",
      "epoch 73: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.32it/s, loss=0.505, metrics={'BinaryAUROC': 0.6435, 'f1': 0.0133, 'BinaryAveragePrecision': 0.3264}] \n",
      "epoch 74: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.76it/s, loss=0.504, metrics={'BinaryAUROC': 0.6471, 'f1': 0.0104, 'BinaryAveragePrecision': 0.3295}]\n",
      "epoch 75: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.24it/s, loss=0.505, metrics={'BinaryAUROC': 0.6419, 'f1': 0.0153, 'BinaryAveragePrecision': 0.3254}]\n",
      "epoch 76: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 95.79it/s, loss=0.505, metrics={'BinaryAUROC': 0.6454, 'f1': 0.0173, 'BinaryAveragePrecision': 0.3286}] \n",
      "epoch 77: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 96.21it/s, loss=0.505, metrics={'BinaryAUROC': 0.6443, 'f1': 0.0137, 'BinaryAveragePrecision': 0.3256}] \n",
      "epoch 78: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.10it/s, loss=0.505, metrics={'BinaryAUROC': 0.6449, 'f1': 0.0178, 'BinaryAveragePrecision': 0.3293}]\n",
      "epoch 79: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 101.46it/s, loss=0.506, metrics={'BinaryAUROC': 0.6447, 'f1': 0.01, 'BinaryAveragePrecision': 0.3206}]  \n",
      "epoch 80: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.37it/s, loss=0.503, metrics={'BinaryAUROC': 0.6486, 'f1': 0.0125, 'BinaryAveragePrecision': 0.3268}]\n",
      "epoch 81: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.97it/s, loss=0.503, metrics={'BinaryAUROC': 0.6485, 'f1': 0.0145, 'BinaryAveragePrecision': 0.3321}]\n",
      "epoch 82: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.39it/s, loss=0.505, metrics={'BinaryAUROC': 0.6453, 'f1': 0.0161, 'BinaryAveragePrecision': 0.3267}]\n",
      "epoch 83: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.32it/s, loss=0.504, metrics={'BinaryAUROC': 0.6469, 'f1': 0.0182, 'BinaryAveragePrecision': 0.3297}]\n",
      "epoch 84: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.05it/s, loss=0.505, metrics={'BinaryAUROC': 0.6436, 'f1': 0.019, 'BinaryAveragePrecision': 0.3253}] \n",
      "epoch 85: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.87it/s, loss=0.504, metrics={'BinaryAUROC': 0.6459, 'f1': 0.017, 'BinaryAveragePrecision': 0.3323}] \n",
      "epoch 86: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 99.66it/s, loss=0.503, metrics={'BinaryAUROC': 0.6477, 'f1': 0.0234, 'BinaryAveragePrecision': 0.3343}] \n",
      "epoch 87: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.68it/s, loss=0.505, metrics={'BinaryAUROC': 0.6448, 'f1': 0.0178, 'BinaryAveragePrecision': 0.3256}]\n",
      "epoch 88: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.70it/s, loss=0.503, metrics={'BinaryAUROC': 0.6502, 'f1': 0.0166, 'BinaryAveragePrecision': 0.332}] \n",
      "epoch 89: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 103.81it/s, loss=0.505, metrics={'BinaryAUROC': 0.6399, 'f1': 0.0145, 'BinaryAveragePrecision': 0.3215}]\n",
      "epoch 90: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 106.38it/s, loss=0.503, metrics={'BinaryAUROC': 0.6484, 'f1': 0.017, 'BinaryAveragePrecision': 0.3324}] \n",
      "epoch 91: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 105.60it/s, loss=0.502, metrics={'BinaryAUROC': 0.6502, 'f1': 0.0149, 'BinaryAveragePrecision': 0.3304}]\n",
      "epoch 92: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.99it/s, loss=0.502, metrics={'BinaryAUROC': 0.6508, 'f1': 0.0185, 'BinaryAveragePrecision': 0.3319}]\n",
      "epoch 93: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.65it/s, loss=0.503, metrics={'BinaryAUROC': 0.6487, 'f1': 0.0157, 'BinaryAveragePrecision': 0.3304}]\n",
      "epoch 94: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.74it/s, loss=0.504, metrics={'BinaryAUROC': 0.647, 'f1': 0.021, 'BinaryAveragePrecision': 0.3297}]  \n",
      "epoch 95: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.48it/s, loss=0.504, metrics={'BinaryAUROC': 0.6478, 'f1': 0.0161, 'BinaryAveragePrecision': 0.3274}]\n",
      "epoch 96: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.53it/s, loss=0.502, metrics={'BinaryAUROC': 0.6534, 'f1': 0.0218, 'BinaryAveragePrecision': 0.3348}]\n",
      "epoch 97: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 100.01it/s, loss=0.505, metrics={'BinaryAUROC': 0.6439, 'f1': 0.0193, 'BinaryAveragePrecision': 0.3281}]\n",
      "epoch 98: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 102.70it/s, loss=0.504, metrics={'BinaryAUROC': 0.6504, 'f1': 0.0185, 'BinaryAveragePrecision': 0.3293}]\n",
      "epoch 99: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 104.61it/s, loss=0.503, metrics={'BinaryAUROC': 0.6501, 'f1': 0.0198, 'BinaryAveragePrecision': 0.3314}]\n",
      "epoch 100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 85/85 [00:00<00:00, 98.76it/s, loss=0.504, metrics={'BinaryAUROC': 0.6484, 'f1': 0.0182, 'BinaryAveragePrecision': 0.3299}] \n"
     ]
    }
   ],
   "source": [
    "# train and validate\n",
    "trainer = Trainer(model, objective=\"binary\", accelerator=\"gpu\",\\\n",
    "                  metrics=[AUROC(task='binary'), F1Score, AveragePrecision(task='binary')])\n",
    "trainer.fit(\n",
    "    X_tab=X_tab,\n",
    "    target=target,\n",
    "    n_epochs=100,\n",
    "    batch_size=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 54.36it/s]\n",
      "predict: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24/24 [00:00<00:00, 57.68it/s]\n"
     ]
    }
   ],
   "source": [
    "# predict on test\n",
    "X_tab_te = tab_preprocessor.transform(test_df)\n",
    "preds = trainer.predict(X_tab=X_tab_te)\n",
    "pred_probs = trainer.predict_proba(X_tab=X_tab_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC:0.6810110658053637\n",
      "PrecisionRecall-AUC:0.3596077008591941\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "target = lukup[name]\n",
    "y = test_df[target].values\n",
    "print(\"ROC-AUC:{}\".format(roc_auc_score(y, pred_probs[:, 1])))\n",
    "print(\"PrecisionRecall-AUC:{}\".format(average_precision_score(y, pred_probs[:, 1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_widedeep import Tab2Vec\n",
    "t2v = Tab2Vec(model=model, tab_preprocessor=tab_preprocessor)\n",
    "X_vec, y = t2v.transform(train_df, target_col=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.4172393e-01, -2.9816014e-01,  2.1386049e+00, ...,\n",
       "         5.1322991e-01,  8.6423665e-01, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         5.5625874e-01,  4.9738172e-01,  8.4056890e-01],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "         6.8739414e-01, -1.5589465e-01,  2.2773863e-01],\n",
       "       ...,\n",
       "       [-1.4172393e-01, -2.9816014e-01, -4.5036829e-01, ...,\n",
       "        -5.1062021e-02, -8.0248013e-02, -8.1868723e-02],\n",
       "       [-1.8721935e+00,  8.3828169e-01,  2.1386049e+00, ...,\n",
       "         3.1717189e-02,  1.0486166e-03, -3.0246025e-03],\n",
       "       [-1.8721935e+00,  8.3828169e-01, -4.5036829e-01, ...,\n",
       "        -6.6634342e-02, -7.7423006e-02, -9.5329911e-02]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_vec"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_03_2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
